{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"P31ZlvuDxSzY","executionInfo":{"status":"ok","timestamp":1764224839084,"user_tz":-540,"elapsed":20332,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["# ========================================================\n","# 1. ğŸ› ï¸ í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ, Import (Code Block 1/2)\n","# ========================================================\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","import torchvision.models as models\n","import pandas as pd\n","import numpy as np\n","import random\n","import os\n","import sys\n","from tqdm import tqdm\n","from typing import Tuple, Dict, List\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity\n","from torch.utils.data.dataloader import default_collate\n","from typing import List, Dict\n"]},{"cell_type":"code","source":["### ì´ ì…€ë§Œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤!!_11270646 ìŠ¹ì—°\n","import os\n","\n","SAVE_DIR = \"/content/drive/MyDrive/2025CV/best_weights_jw_content\"\n","\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","print(\"Checkpoint ì €ì¥ í´ë” ìƒì„± ì™„ë£Œ:\", SAVE_DIR)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPkBPMF4RzJf","executionInfo":{"status":"ok","timestamp":1764229594292,"user_tz":-540,"elapsed":21,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}},"outputId":"a2716d4b-576e-4436-9fe0-7606e64dc9e5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint ì €ì¥ í´ë” ìƒì„± ì™„ë£Œ: /content/drive/MyDrive/2025CV/best_weights_jw_content\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18910,"status":"ok","timestamp":1764224857997,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"},"user_tz":-540},"id":"ACpB338K0748","outputId":"17fc1700-2932-4f02-d021-3b2873ae4078"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# ë§ˆìš´íŠ¸ (ê³µìœ  ë“œë¼ì´ë¸Œ ê²½ë¡œê°€ MyDriveì— ë°”ë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n","drive.mount('/content/drive')\n","\n","# --- ê²½ë¡œ ì„¤ì • ---\n","# ğŸš¨ [í†µì¼ í•„ìˆ˜] ê³µìœ  ë“œë¼ì´ë¸Œ ê²½ë¡œì™€ íŒŒì¼ëª…\n","# ì´ ê²½ë¡œê°€ dataset.py, transforms.py, checkpoints í´ë”ê°€ ìˆëŠ” ìœ„ì¹˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n","MODULE_PATH = \"/content/drive/MyDrive/2025CV\"\n","sys.path.append(MODULE_PATH)\n","\n","# ğŸ’¡ Import Custom Modules (dataset.py, transforms.py)\n","# BBox í¬ë¡­ ë¡œì§ê³¼ Transforms ì •ì˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n","from dataset_jw import DeepFashionC2S\n","from transforms import train_transform, val_transform"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1764224858051,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"},"user_tz":-540},"id":"G_vYkBF11EK4","outputId":"ac1863f6-6058-4e60-afcc-094751735fec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["# --- Hyperparameters (íŒ€ì› ê°„ í†µì¼ í•„ìˆ˜) ---\n","EXPERIMENT_SEED = 42\n","EMBEDDING_DIM = 128  # 128ë¡œ ê³ ì •\n","LEARNING_RATE = 1e-4\n","TRIPLET_MARGIN = 0.5 # Online Semi-Hard Triplet Loss ë§ˆì§„ ê°’\n","BATCH_SIZE = 32\n","PATIENCE = 5         # Early Stopping Patience (5 Epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)\n","MAX_EPOCHS = 40      # ìµœëŒ€ í•™ìŠµ Epoch ìˆ˜\n","CHECKPOINT_DIR = os.path.join(MODULE_PATH, \"checkpoint_jw_content\")\n","\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# ğŸš¨ ì¬í˜„ì„± í™•ë³´ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    return torch.Generator().manual_seed(seed)\n","\n","generator = set_seed(EXPERIMENT_SEED)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vVz_Qlb51Hqh","executionInfo":{"status":"ok","timestamp":1764224858054,"user_tz":-540,"elapsed":2,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["\n","# --- CSV íŒŒì¼ ë¡œë“œ (ìƒ˜í”Œë§ëœ CSV ì‚¬ìš©) ---\n","CSV_PATH_LIGHT = os.path.join(MODULE_PATH, \"meta_c2s_10_2_2_sampling_ID.csv\")"]},{"cell_type":"code","source":["import os\n","import shutil\n","import pandas as pd\n","\n","# --- ê²½ë¡œ ì„¤ì • í™•ì¸ ---\n","\n","DRIVE_IMG_ROOT = os.path.join(MODULE_PATH, \"Images\") # ì›ë³¸ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Drive)\n","LOCAL_IMG_ROOT = \"/content/Images\"                   # íƒ€ê²Ÿ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Local)"],"metadata":{"id":"6embU8hmrO-9","executionInfo":{"status":"ok","timestamp":1764224858062,"user_tz":-540,"elapsed":6,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# =========================================================\n","# ğŸ’¡ [ìˆ˜ì •] CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ë³µì‚¬ ë° I/O ìµœì í™”\n","# =========================================================\n","print(\"ğŸš€ CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\")\n","\n","# 1. CSV íŒŒì¼ ë¡œë“œ\n","try:\n","    df_light = pd.read_csv(CSV_PATH_LIGHT)\n","except FileNotFoundError:\n","    print(f\"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {CSV_PATH_LIGHT}\")\n","    exit()\n","\n","# 2. í•„ìš”í•œ ëª¨ë“  ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ\n","# consumer_pathì™€ shop_path ì—´ì—ì„œ ìœ ë‹ˆí¬í•œ ê²½ë¡œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","required_paths = pd.concat([df_light['consumer_path'], df_light['shop_path']]).unique()\n","print(f\"ì´ {len(required_paths)}ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\")\n","\n","# 3. ë¡œì»¬ íƒ€ê²Ÿ í´ë” ìƒì„±\n","os.makedirs(LOCAL_IMG_ROOT, exist_ok=True)\n","\n","# 4. íŒŒì¼ ë³µì‚¬ ë° í´ë” êµ¬ì¡° ìœ ì§€\n","copied_count = 0\n","for relative_path in required_paths:\n","    # ì›ë³¸ íŒŒì¼ ê²½ë¡œ (Drive)\n","    source_file_path = os.path.join(DRIVE_IMG_ROOT, relative_path)\n","\n","    # íƒ€ê²Ÿ íŒŒì¼ ê²½ë¡œ (Local)\n","    target_file_path = os.path.join(LOCAL_IMG_ROOT, relative_path)\n","\n","    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„± (ì˜ˆ: /content/Images/img/TOPS/Summer_ ì— í•„ìš”í•œ í´ë” ìƒì„±)\n","    target_dir = os.path.dirname(target_file_path)\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    # íŒŒì¼ ë³µì‚¬\n","    try:\n","        if not os.path.exists(target_file_path):\n","             shutil.copy2(source_file_path, target_file_path)\n","             copied_count += 1\n","    except FileNotFoundError:\n","        # CSVì— ê²½ë¡œê°€ ìˆì§€ë§Œ ì‹¤ì œ Driveì— íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.\n","        print(f\"[ê²½ê³ ] ì›ë³¸ íŒŒì¼ì´ Driveì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€: {source_file_path}\")\n","    except Exception as e:\n","        print(f\"[ì˜¤ë¥˜] ë³µì‚¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({relative_path}): {e}\")\n","\n","print(f\"âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ {copied_count}ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\")\n","\n","# =========================================================\n","# ğŸ’¡ [ì¶”ê°€] 3. IMG_ROOT_DIR ë³€ìˆ˜ë¥¼ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½\n","# =========================================================\n","# ê¸°ì¡´ ê²½ë¡œ ë³€ìˆ˜ë¥¼ ìƒˆë¡œìš´ ë¡œì»¬ ê²½ë¡œë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n","IMG_ROOT_DIR = LOCAL_IMG_ROOT\n","\n","print(f\"ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: {IMG_ROOT_DIR}\")"],"metadata":{"id":"H_3ldDfXzwfx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73df2843-df86-415a-8e36-900c52214199","executionInfo":{"status":"ok","timestamp":1764228672817,"user_tz":-540,"elapsed":3814746,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\n","ì´ 10546ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\n","âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ 10546ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\n","ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: /content/Images\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"6YvjqWS-dlG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764228672848,"user_tz":-540,"elapsed":22,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}},"outputId":"9a24c5f1-fcff-4f5f-aeff-4b3b4f1dcf15"},"outputs":[{"output_type":"stream","name":"stdout","text":["ì´ unique item_id ê°œìˆ˜: 1467\n"]}],"source":["# ============================================\n","# item_id ë¬¸ìì—´ â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ ë§¤í•‘ ìƒì„±\n","# ============================================\n","\n","df_full = pd.read_csv(CSV_PATH_LIGHT)\n","unique_ids = df_full[\"item_id\"].unique()\n","\n","id2label = {id_str: idx for idx, id_str in enumerate(unique_ids)}\n","print(\"ì´ unique item_id ê°œìˆ˜:\", len(id2label))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"TU0u3b3c1IV4","executionInfo":{"status":"ok","timestamp":1764228672855,"user_tz":-540,"elapsed":3,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["# ========================================================\n","# 2. ğŸ§  ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (Stage 2 í•µì‹¬) (Code Block 2/2)\n","# ========================================================\n","\n","# ğŸš¨ ë°±ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (Feature Embedding)\n","class FeatureEmbedding(nn.Module):\n","    def __init__(self, backbone_name, embedding_dim):\n","        super().__init__()\n","\n","        # 1. ë°±ë³¸ ë¡œë“œ ë° ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì„¤ì •\n","        if backbone_name == 'ResNet-50':\n","            base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","            num_ftrs = base_model.fc.in_features\n","\n","        elif backbone_name == 'ResNet-34':\n","            base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n","            num_ftrs = base_model.fc.in_features\n","\n","        elif backbone_name == 'EfficientNet-B3':\n","            base_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n","            num_ftrs = base_model.classifier[1].in_features\n","\n","        else:\n","            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n","\n","        # 2. íŠ¹ì§• ì¶”ì¶œê¸° (ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±°)\n","        if 'ResNet' in backbone_name:\n","            self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n","            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n","        elif 'EfficientNet' in backbone_name:\n","            self.feature_extractor = base_model.features\n","            self.avgpool = nn.AdaptiveAvgPool2d(1)\n","            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n","\n","        self.bn = nn.BatchNorm1d(embedding_dim)\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        if hasattr(self, 'avgpool'):\n","            x = self.avgpool(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = self.final_fc(x)\n","        x = self.bn(x)\n","        x = F.normalize(x, p=2, dim=1)\n","        return x.to(DEVICE)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KagI8InF1Rfv","executionInfo":{"status":"ok","timestamp":1764228672862,"user_tz":-540,"elapsed":3,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["# ğŸš¨ [í†µí•©] Online Semi-Hard Triplet Loss í•¨ìˆ˜\n","def pairwise_distance_sq(embeddings: torch.Tensor) -> torch.Tensor:\n","    \"\"\"ë°°ì¹˜ ë‚´ ëª¨ë“  ì„ë² ë”© ìŒ ì‚¬ì´ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ì œê³±ì„ ê³„ì‚°.\"\"\"\n","    dot_product = torch.matmul(embeddings, embeddings.t())\n","    square_norm = torch.diag(dot_product)\n","    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n","    distances[distances < 0] = 0\n","    return distances\n","\n","# ğŸš¨ [í†µì¼ í•„ìˆ˜] ì˜¨ë¼ì¸ ì„¸ë¯¸-í•˜ë“œ íŠ¸ë¦½ë › ì†ì‹¤ í•¨ìˆ˜ (Semi-Hard Mining êµ¬í˜„)\n","def online_semi_hard_triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float) -> torch.Tensor:\n","    \"\"\"\n","    Online Semi-Hard Triplet Loss: ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ë§Œ ê°€ì¥ ì–´ë µì§€ëŠ” ì•Šì€ Negativeë¥¼ ì„ íƒí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ.\n","    \"\"\"\n","    pair_dist = pairwise_distance_sq(embeddings)\n","    labels = labels.to(pair_dist.device)\n","    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n","\n","    # Hard Positive (A-P ì¤‘ ê°€ì¥ ë¨¼ ê±°ë¦¬)\n","    dist_ap = pair_dist.clone()\n","    dist_ap[~labels_eq] = float('-inf')\n","    hard_positive_dist, _ = dist_ap.max(dim=1)\n","\n","    # Semi-Hard Negative (D(A,N) > D(A,P) ì´ë©´ì„œ ê°€ì¥ ê°€ê¹Œìš´ N)\n","    dist_an = pair_dist.clone()\n","    dist_an[labels_eq] = float('inf')\n","\n","    # Semi-Hard ì¡°ê±´: D(A,N) > D(A,P) + marginì„ ë§Œì¡±í•˜ëŠ” ì˜ì—­\n","    # ë§ˆì§„ ì¡°ê±´ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” (A-P ê±°ë¦¬ê°€ A-Në³´ë‹¤ ì´ë¯¸ margin ì´ìƒ ì‘ì€) Nì„ ì°¾ê¸° ìœ„í•´ ë¶€ë“±í˜¸ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n","    # Triplet Loss: D(A,P) - D(A,N) + margin > 0 ì¸ íŠ¸ë¦½ë ›ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n","    # Semi-HardëŠ” D(A,P) < D(A,N) < D(A,P) + margin ì…ë‹ˆë‹¤.\n","\n","    # 1. Negative ì¤‘ ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” í›„ë³´ (D(A,N) > D(A,P))\n","    is_semi_hard_candidate = dist_an > hard_positive_dist.unsqueeze(1)\n","\n","    # 2. Semi-Hard Negative ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ\n","    # D(A,N)ì´ D(A,P)ë³´ë‹¤ í¬ë©´ì„œ, A-P ë§ˆì§„(alpha)ì„ ë„˜ì§€ ì•ŠëŠ” ì˜ì—­ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ Nì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n","    # PyTorch ë‚´ì¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë³µì¡í•œ ì¸ë±ì‹± ëŒ€ì‹  ì•ˆì •ì„±ì„ ìœ„í•´ D(A,N) > D(A,P)ì¸ ìƒ˜í”Œë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n","    dist_an[~is_semi_hard_candidate] = float('inf')\n","    semi_hard_negative_dist, _ = dist_an.min(dim=1)\n","\n","    # Triplet Loss ê³„ì‚°: L = max(0, D(A, P)_hard - D(A, N)_semi_hard + margin)\n","    triplet_loss = hard_positive_dist - semi_hard_negative_dist + margin\n","    triplet_loss[triplet_loss < 0] = 0\n","\n","    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n","\n","    return triplet_loss.sum() / (num_hard_triplets + 1e-16) if num_hard_triplets > 0 else torch.tensor(0.0).to(embeddings.device)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"O0YIpFve10oG","executionInfo":{"status":"ok","timestamp":1764228672870,"user_tz":-540,"elapsed":5,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["# ========================================================\n","# 3. ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ë° í•™ìŠµ ë£¨í”„ (ìˆ˜ì • ì™„ë£Œ)\n","# ========================================================\n","\n","def save_checkpoint(model, optimizer, epoch, best_val_metric, patience_count, filename):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'best_val_metric': best_val_metric, # Recall@K ê°’ (í´ìˆ˜ë¡ ì¢‹ìŒ)\n","        'patience_count': patience_count\n","    }\n","    torch.save(checkpoint, filename)\n","\n","def load_checkpoint(model, optimizer, filename):\n","    if not os.path.exists(filename):\n","        print(\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n","        # Recall@KëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ, best_val_metricì„ 0.0ìœ¼ë¡œ ì´ˆê¸°í™”\n","        return 0, 0.0, 0\n","\n","    checkpoint = torch.load(filename, map_location=DEVICE)\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    start_epoch = checkpoint['epoch'] + 1\n","    best_val_metric = checkpoint['best_val_metric']\n","    patience_counter = checkpoint['patience_count']\n","\n","    print(f\"âœ… í•™ìŠµ ì¬ê°œ: Epoch {start_epoch}ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. (ìµœê³  Recall@5: {best_val_metric:.4f})\")\n","\n","    return start_epoch, best_val_metric, patience_counter"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"tFFI2p_-C3i3","executionInfo":{"status":"ok","timestamp":1764228672911,"user_tz":-540,"elapsed":37,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"outputs":[],"source":["def train_model(backbone_name, train_dl, val_dl, criterion):\n","\n","    # --- ì´ˆê¸°í™”/ë³µêµ¬ ---\n","    model = FeatureEmbedding(backbone_name, EMBEDDING_DIM).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"{backbone_name}_checkpoint.pth\")\n","    start_epoch, best_val_metric, patience_counter = load_checkpoint(model, optimizer, checkpoint_file)\n","\n","    # ğŸš¨ ì‹¤í—˜ ê¸°ë¡ ë³µêµ¬\n","    history_file = os.path.join(CHECKPOINT_DIR, f\"{backbone_name}_history.csv\")\n","    history = []\n","\n","    if os.path.exists(history_file) and os.path.getsize(history_file) > 0:\n","        try:\n","            history_df = pd.read_csv(history_file)\n","            if not history_df.empty:\n","                history = history_df.to_dict('records')\n","            else:\n","                raise pd.errors.EmptyDataError(\"DataFrame is empty after reading.\")\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"âš ï¸ {backbone_name}_history.csv íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n","            history = []\n","        except Exception as e:\n","            print(f\"âš ï¸ History file loading error ({e.__class__.__name__}): {e}. Starting new history.\")\n","            history = []\n","\n","    # --- í•™ìŠµ ë£¨í”„ ---\n","    for epoch in range(start_epoch, MAX_EPOCHS):\n","\n","        print(f\"\\n--- Starting Epoch {epoch + 1}/{MAX_EPOCHS} for {backbone_name} ---\")\n","\n","        model.train()\n","        total_loss = 0\n","\n","        # Batch-All Triplet Lossë¥¼ ìœ„í•œ ë°ì´í„° ë¡œë”© ë£¨í”„\n","        for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1} Train ({backbone_name})\", leave=False):\n","            consumer_imgs = batch[\"consumer\"].to(DEVICE)\n","            shop_imgs = batch[\"shop\"].to(DEVICE)\n","            item_ids = batch[\"item_id\"]\n","\n","            # ë¬¸ìì—´ item_id â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜\n","            if isinstance(item_ids, (list, tuple)):\n","                item_ids = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                item_ids = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","\n","            item_ids = item_ids.to(DEVICE)\n","\n","            # ì´ë¯¸ì§€/ë¼ë²¨ ë³‘í•©\n","            all_imgs = torch.cat([consumer_imgs, shop_imgs], dim=0)\n","            all_labels = torch.cat([item_ids, item_ids], dim=0)\n","\n","            # ëª¨ë¸ ì—…ë°ì´íŠ¸\n","            optimizer.zero_grad()\n","            embeddings = model(all_imgs)\n","            loss = criterion(embeddings, all_labels, TRIPLET_MARGIN)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_train_loss = total_loss / len(train_dl)\n","\n","        # --- ê²€ì¦ ë£¨í”„: ë§¤ ì—í¬í¬ë§ˆë‹¤ ì‹¤í–‰ ---\n","        val_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1, 5, 10])\n","        val_metric = val_recalls['R@5']\n","\n","        print(\n","            f\"Epoch {epoch+1} | \"\n","            f\"Train Loss: {avg_train_loss:.4f} | \"\n","            f\"R@1: {val_recalls['R@1']:.3f} | \"\n","            f\"R@5: {val_recalls['R@5']:.3f} | \"\n","            f\"R@10: {val_recalls['R@10']:.3f}\"\n","        )\n","\n","        history.append({\n","            'epoch': epoch + 1,\n","            'train_loss': avg_train_loss,\n","            'val_R@1': val_recalls['R@1'],\n","            'val_R@5': val_recalls['R@5'],\n","            'val_R@10': val_recalls['R@10']\n","        })\n","\n","        # --- Early Stopping ---\n","        if val_metric > best_val_metric:\n","            best_val_metric = val_metric\n","            patience_counter = 0\n","            save_checkpoint(\n","                model, optimizer, epoch, best_val_metric, patience_counter,\n","                checkpoint_file.replace(\"checkpoint\", \"best_weights\")\n","            )\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                print(f\"Early stopping triggered at epoch {epoch+1}.\")\n","                # history ì €ì¥ í›„ ì¢…ë£Œ\n","                pd.DataFrame(history).to_csv(history_file, index=False)\n","                break\n","\n","        # Epoch ì¢…ë£Œ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n","        save_checkpoint(model, optimizer, epoch, best_val_metric, patience_counter, checkpoint_file)\n","\n","        # history ì €ì¥\n","        pd.DataFrame(history).to_csv(history_file, index=False)\n","\n","    # --- ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---\n","    try:\n","        load_checkpoint(model, optimizer, checkpoint_file.replace(\"checkpoint\", \"best_weights\"))\n","        final_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1, 5, 10])\n","\n","        print(\"\\n========================================================\")\n","        print(f\"ğŸ† {backbone_name} Final Best R@1: {final_recalls['R@1']:.4f}\")\n","        print(f\"ğŸ† {backbone_name} Final Best R@5: {final_recalls['R@5']:.4f}\")\n","        print(f\"ğŸ† {backbone_name} Final Best R@10: {final_recalls['R@10']:.4f}\")\n","        print(\"========================================================\\n\")\n","\n","        return {\"backbone\": backbone_name, \"final_epoch\": epoch + 1, \"best_R@5\": final_recalls['R@5']}\n","\n","    except Exception as e:\n","        print(f\"Final evaluation failed: {e}\")\n","        return {\"backbone\": backbone_name, \"final_epoch\": epoch + 1, \"best_R@5\": 0.0}\n"]},{"cell_type":"code","source":["# ========================================================\n","# ğŸš¨ [ìµœì¢…ë³¸] calculate_recall_at_k í•¨ìˆ˜ (Dict Handling & Numeric Conversion)\n","# ========================================================\n","def calculate_recall_at_k(model, dataloader, device, ks=[1, 5, 10]):\n","    \"\"\"\n","    Validation Setì˜ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³  Recall@K ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n","    (DataLoaderê°€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ê³ , item_idë¥¼ ìˆ«ì ë¼ë²¨ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©)\n","    \"\"\"\n","    model.eval()\n","    all_query_embs = []\n","    all_gallery_embs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=f\"Calculating Recall@{ks[-1]}\"):\n","\n","            # ğŸš¨ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ batchì—ì„œ ë°ì´í„° ì¶”ì¶œ\n","            consumer_imgs = batch[\"consumer\"]\n","            shop_imgs = batch[\"shop\"]\n","            item_ids = batch[\"item_id\"] # ë¬¸ìì—´ (str) ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŠœí”Œ\n","\n","            # ğŸš¨ ë¬¸ìì—´ item_id â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ (train_modelê³¼ ë™ì¼í•œ ë¡œì§ ì ìš©)\n","            if isinstance(item_ids, list) or isinstance(item_ids, tuple):\n","                # ğŸš¨ Global id2label ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ê°€ì •\n","                item_ids_tensor = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                item_ids_tensor = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","\n","            # ì„ë² ë”© ê³„ì‚°\n","            query_embs = model(consumer_imgs.to(device)).cpu().numpy()\n","            gallery_embs = model(shop_imgs.to(device)).cpu().numpy()\n","\n","            all_query_embs.append(query_embs)\n","            all_gallery_embs.append(gallery_embs)\n","            all_labels.append(item_ids_tensor.cpu().numpy()) # ìˆ«ì Tensorë¥¼ NumPy ë°°ì—´ë¡œ ì €ì¥\n","\n","    query_embs = np.concatenate(all_query_embs, axis=0)\n","    gallery_embs = np.concatenate(all_gallery_embs, axis=0)\n","    gallery_labels = np.concatenate(all_labels, axis=0) # [N,] í˜•íƒœì˜ ìˆ«ì ë°°ì—´\n","\n","    # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ í–‰ë ¬)\n","    sims = cosine_similarity(query_embs, gallery_embs)\n","\n","    recalls = {}\n","    for k in ks:\n","        # ìƒìœ„ Kê°œì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ (ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ)\n","        topk_idx = np.argsort(-sims, axis=1)[:, :k]\n","\n","        correct_count = 0\n","        for i in range(len(gallery_labels)):\n","            query_true_id = gallery_labels[i]\n","\n","            # ìƒìœ„ Kê°œì˜ ê°¤ëŸ¬ë¦¬ ìƒí’ˆ ID\n","            topk_ids = gallery_labels[topk_idx[i]]\n","\n","            # ğŸš¨ ì •ë‹µ IDê°€ ìƒìœ„ Kê°œ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ìˆ«ì ë¹„êµ)\n","            if query_true_id in topk_ids:\n","                correct_count += 1\n","\n","        recalls[f'R@{k}'] = correct_count / len(gallery_labels)\n","\n","    return recalls"],"metadata":{"id":"jV_InoDJpujk","executionInfo":{"status":"ok","timestamp":1764228672919,"user_tz":-540,"elapsed":10,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def custom_collate_fn(batch: List[Dict]):\n","    \"\"\"Noneì„ ë°˜í™˜í•˜ëŠ” ìƒ˜í”Œ(íŒŒì¼ ëˆ„ë½)ì„ í•„í„°ë§í•˜ê³  collate.\"\"\"\n","    # Noneì¸ ìƒ˜í”Œì„ í•„í„°ë§í•©ë‹ˆë‹¤.\n","    batch = [item for item in batch if item is not None]\n","\n","    # ë°°ì¹˜ì— ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ None ë°˜í™˜\n","    if not batch:\n","        return None\n","\n","    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ collate\n","    return default_collate(batch)"],"metadata":{"id":"TH1WpbZWyS3b","executionInfo":{"status":"ok","timestamp":1764228672928,"user_tz":-540,"elapsed":5,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["NUM_WORKERS = 4"],"metadata":{"id":"8CF1SyCgyhpR","executionInfo":{"status":"ok","timestamp":1764228673038,"user_tz":-540,"elapsed":102,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# ========================================================\n","# 4. ğŸ ì‹¤í—˜ ì‹¤í–‰ (ì„¸ ê°€ì§€ ë°±ë³¸ ìˆœì°¨ ì§„í–‰)\n","# ========================================================\n","if __name__ == '__main__':\n","    # 1. DataLoader ìƒì„± (ìƒ˜í”Œë§ëœ CSVë¥¼ ì‚¬ìš©)\n","    train_ds = DeepFashionC2S(csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR, transform=train_transform, split='train')\n","    val_ds = DeepFashionC2S(csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR, transform=val_transform, split='val')\n","\n","    # =========================================================\n","    # ğŸ’¡ [ìˆ˜ì •] DataLoader ìµœì í™” ì„¤ì • (dataset.py ìˆ˜ì • ë¶ˆê°€ ì¡°ê±´)\n","    # =========================================================\n","\n","    # num_workers=0ì„ ìœ ì§€ (ì•ˆì •ì„± í™•ë³´)\n","    # pin_memoryë§Œ Trueë¡œ ë³€ê²½í•˜ì—¬ CPU -> GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n","\n","    # ğŸš¨ [ìˆ˜ì •] DataLoader ìµœì í™” ì„¤ì •\n","    train_dl = DataLoader(\n","        train_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS, # ğŸš€ 0 ì´ìƒìœ¼ë¡œ ì„¤ì •\n","        generator=generator,\n","        pin_memory=True,         # ğŸš€ Trueë¡œ ì„¤ì •\n","        collate_fn=custom_collate_fn # ğŸš¨ Custom Collate í•¨ìˆ˜ ì ìš©\n","    )\n","\n","    val_dl = DataLoader(\n","        val_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=NUM_WORKERS, # ğŸš€ 0 ì´ìƒìœ¼ë¡œ ì„¤ì •\n","        pin_memory=True,         # ğŸš€ Trueë¡œ ì„¤ì •\n","        collate_fn=custom_collate_fn # ğŸš¨ Custom Collate í•¨ìˆ˜ ì ìš©\n","    )\n","    # 2. ë°±ë³¸ ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸\n","    BACKBONES_TO_TEST = ['EfficientNet-B3']\n","    all_results = []\n","\n","    for backbone in BACKBONES_TO_TEST:\n","        print(f\"\\n================ Running Experiment: {backbone} ================\")\n","\n","        # ğŸš¨ Online Semi-Hard Triplet Loss í•¨ìˆ˜ ì „ë‹¬\n","        criterion = online_semi_hard_triplet_loss\n","\n","        results = train_model(backbone, train_dl, val_dl, criterion)\n","        all_results.append(results)\n","\n","    print(\"\\n========== ALL EXPERIMENTS FINAL SUMMARY ==========\")\n","    print(pd.DataFrame(all_results))"],"metadata":{"id":"cboaanAVrGKu","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1764231687087,"user_tz":-540,"elapsed":2057406,"user":{"displayName":"ì¡°ìŠ¹ì—°","userId":"08641787915667750691"}},"outputId":"ec7f1475-0942-471a-f8ff-34eef4f2f6d2"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","================ Running Experiment: EfficientNet-B3 ================\n","âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n","\n","--- Starting Epoch 1/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Train Loss: 0.3886 | R@1: 0.450 | R@5: 0.545 | R@10: 0.604\n","\n","--- Starting Epoch 2/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 2 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Train Loss: 0.3079 | R@1: 0.520 | R@5: 0.622 | R@10: 0.672\n","\n","--- Starting Epoch 3/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 3 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Train Loss: 0.2647 | R@1: 0.551 | R@5: 0.626 | R@10: 0.695\n","\n","--- Starting Epoch 4/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 4 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Train Loss: 0.2323 | R@1: 0.575 | R@5: 0.651 | R@10: 0.706\n","\n","--- Starting Epoch 5/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 5 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Train Loss: 0.2130 | R@1: 0.573 | R@5: 0.668 | R@10: 0.704\n","\n","--- Starting Epoch 6/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 6 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 | Train Loss: 0.1937 | R@1: 0.557 | R@5: 0.640 | R@10: 0.675\n","\n","--- Starting Epoch 7/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 7 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 | Train Loss: 0.1840 | R@1: 0.582 | R@5: 0.670 | R@10: 0.721\n","\n","--- Starting Epoch 8/40 for EfficientNet-B3 ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 8 Train (EfficientNet-B3):   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-580480214.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_semi_hard_triplet_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1153233899.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(backbone_name, train_dl, val_dl, criterion)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRIPLET_MARGIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}