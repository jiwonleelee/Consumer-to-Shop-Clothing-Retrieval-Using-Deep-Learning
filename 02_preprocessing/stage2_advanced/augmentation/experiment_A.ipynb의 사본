{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"P31ZlvuDxSzY","executionInfo":{"status":"ok","timestamp":1765129063775,"user_tz":-540,"elapsed":17198,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["13-2# 1. í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ, Import\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Subset\n","import torchvision.models as models\n","import pandas as pd\n","import numpy as np\n","import random\n","import os\n","import sys\n","from tqdm import tqdm\n","from PIL import Image\n","from typing import Tuple, Dict, List\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity\n","from torch.utils.data.dataloader import default_collate\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ACpB338K0748","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765129157743,"user_tz":-540,"elapsed":93964,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}},"outputId":"bf103444-c085-4c19-b14d-7e28bba3f175"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# ë§ˆìš´íŠ¸ (ê³µìœ  ë“œë¼ì´ë¸Œ ê²½ë¡œê°€ MyDriveì— ë°”ë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n","drive.mount('/content/drive')\n","\n","# --- ê²½ë¡œ ì„¤ì • ---\n","# ì´ ê²½ë¡œê°€ dataset.py, transforms.py, checkpoints í´ë”ê°€ ìˆëŠ” ìœ„ì¹˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n","MODULE_PATH = \"/content/drive/MyDrive/2025CV\"\n","sys.path.append(MODULE_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"t5XkGI2OcSbo","executionInfo":{"status":"ok","timestamp":1765129159073,"user_tz":-540,"elapsed":1332,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["# Import Custom Modules (dataset.py, transforms.py)\n","# BBox í¬ë¡­ ë¡œì§ê³¼ Transforms ì •ì˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n","from dataset_jw import DeepFashionC2S\n","# from transforms import train_transform, val_transform"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1765129159127,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"},"user_tz":-540},"id":"G_vYkBF11EK4","outputId":"8da72a49-a382-4141-a436-8a83cd4f08d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["# --- Hyperparameters (íŒ€ì› ê°„ í†µì¼ í•„ìˆ˜) ---\n","EXPERIMENT_SEED = 42\n","EMBEDDING_DIM = 128  # 128ë¡œ ê³ ì •\n","LEARNING_RATE = 1e-4\n","TRIPLET_MARGIN = 0.5 # Online Semi-Hard Triplet Loss ë§ˆì§„ ê°’\n","BATCH_SIZE = 32\n","PATIENCE = 5         # Early Stopping Patience (5 Epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)\n","MAX_EPOCHS = 40      # ìµœëŒ€ í•™ìŠµ Epoch ìˆ˜\n","CHECKPOINT_DIR = os.path.join(MODULE_PATH, \"checkpoints_A\")\n","\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# ì¬í˜„ì„± í™•ë³´ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    return torch.Generator().manual_seed(seed)\n","\n","generator = set_seed(EXPERIMENT_SEED)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vVz_Qlb51Hqh","executionInfo":{"status":"ok","timestamp":1765129159132,"user_tz":-540,"elapsed":3,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["# --- CSV íŒŒì¼ ë¡œë“œ (ìƒ˜í”Œë§ëœ CSV ì‚¬ìš©) ---\n","CSV_PATH_LIGHT = os.path.join(MODULE_PATH, \"meta_c2s_10_2_2_sampling_ID.csv\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6embU8hmrO-9","executionInfo":{"status":"ok","timestamp":1765129159145,"user_tz":-540,"elapsed":11,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["import os\n","import shutil\n","import pandas as pd\n","\n","# --- ê²½ë¡œ ì„¤ì • í™•ì¸ ---\n","\n","DRIVE_IMG_ROOT = os.path.join(MODULE_PATH, \"Images\") # ì›ë³¸ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Drive)\n","LOCAL_IMG_ROOT = \"/content/Images\"                   # íƒ€ê²Ÿ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Local)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_3ldDfXzwfx","outputId":"aa7bba35-817a-42da-a1dc-b3bdf3164a26","executionInfo":{"status":"ok","timestamp":1765133254909,"user_tz":-540,"elapsed":4095763,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\n","ì´ 10546ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\n","âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ 10546ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\n"]}],"source":["# =========================================================\n","# CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ë³µì‚¬ ë° I/O ìµœì í™”\n","# =========================================================\n","print(\"CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\")\n","\n","# 1. CSV íŒŒì¼ ë¡œë“œ\n","try:\n","    df_light = pd.read_csv(CSV_PATH_LIGHT)\n","except FileNotFoundError:\n","    print(f\"ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {CSV_PATH_LIGHT}\")\n","    exit()\n","\n","# 2. í•„ìš”í•œ ëª¨ë“  ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ\n","# consumer_pathì™€ shop_path ì—´ì—ì„œ ìœ ë‹ˆí¬í•œ ê²½ë¡œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","required_paths = pd.concat([df_light['consumer_path'], df_light['shop_path']]).unique()\n","print(f\"ì´ {len(required_paths)}ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\")\n","\n","# 3. ë¡œì»¬ íƒ€ê²Ÿ í´ë” ìƒì„±\n","os.makedirs(LOCAL_IMG_ROOT, exist_ok=True)\n","\n","# 4. íŒŒì¼ ë³µì‚¬ ë° í´ë” êµ¬ì¡° ìœ ì§€\n","copied_count = 0\n","for relative_path in required_paths:\n","    # ì›ë³¸ íŒŒì¼ ê²½ë¡œ (Drive)\n","    source_file_path = os.path.join(DRIVE_IMG_ROOT, relative_path)\n","\n","    # íƒ€ê²Ÿ íŒŒì¼ ê²½ë¡œ (Local)\n","    target_file_path = os.path.join(LOCAL_IMG_ROOT, relative_path)\n","\n","    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±\n","    target_dir = os.path.dirname(target_file_path)\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    # íŒŒì¼ ë³µì‚¬\n","    try:\n","        if not os.path.exists(target_file_path):\n","             shutil.copy2(source_file_path, target_file_path)\n","             copied_count += 1\n","    except FileNotFoundError:\n","        # CSVì— ê²½ë¡œê°€ ìˆì§€ë§Œ ì‹¤ì œ Driveì— íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.\n","        print(f\"[ê²½ê³ ] ì›ë³¸ íŒŒì¼ì´ Driveì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€: {source_file_path}\")\n","    except Exception as e:\n","        print(f\"[ì˜¤ë¥˜] ë³µì‚¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({relative_path}): {e}\")\n","\n","print(f\"âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ {copied_count}ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"B1Zd89dDrACq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765133254910,"user_tz":-540,"elapsed":9,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}},"outputId":"7e54cf89-7465-4f15-928e-f2cec60f800c"},"outputs":[{"output_type":"stream","name":"stdout","text":["ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: /content/Images\n"]}],"source":["# =========================================================\n","# 3. IMG_ROOT_DIR ë³€ìˆ˜ë¥¼ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½\n","# =========================================================\n","# ê¸°ì¡´ ê²½ë¡œ ë³€ìˆ˜ë¥¼ ìƒˆë¡œìš´ ë¡œì»¬ ê²½ë¡œë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n","IMG_ROOT_DIR = LOCAL_IMG_ROOT\n","\n","print(f\"ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: {IMG_ROOT_DIR}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6YvjqWS-dlG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765133254917,"user_tz":-540,"elapsed":10,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}},"outputId":"a3f89f40-8956-4959-a51b-1ca9bd49c2df"},"outputs":[{"output_type":"stream","name":"stdout","text":["ì´ unique item_id ê°œìˆ˜: 1467\n"]}],"source":["# ============================================\n","# item_id ë¬¸ìì—´ â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ ë§¤í•‘ ìƒì„±\n","# ============================================\n","\n","df_full = pd.read_csv(CSV_PATH_LIGHT)\n","unique_ids = df_full[\"item_id\"].unique()\n","\n","id2label = {id_str: idx for idx, id_str in enumerate(unique_ids)}\n","print(\"ì´ unique item_id ê°œìˆ˜:\", len(id2label))\n"]},{"cell_type":"code","source":["# ============================================\n","# (ì¶”ê°€) Augmentation (Weak / Medium / Strong)\n","# ============================================\n","\n","def build_transforms(aug_mode=\"weak\", train=True, size=224):\n","    ops = []\n","\n","    # Crop / Resize ì „ëµ\n","    if train:\n","        if aug_mode == \"medium\":\n","            ops.append(transforms.RandomResizedCrop(size, scale=(0.8, 1.0)))\n","        elif aug_mode == \"strong\":\n","            ops.append(transforms.RandomResizedCrop(size, scale=(0.6, 1.0)))\n","        elif aug_mode == \"super_strong\":\n","            ops.append(transforms.RandomResizedCrop(size, scale=(0.5, 1.0)))\n","        else: # weak & baseline\n","            ops.append(transforms.Resize((size, size)))\n","    else:\n","        ops.append(transforms.Resize((size, size)))\n","\n","    # ìƒ‰ìƒ/flip ê´€ë ¨ ì¦ê°• (PIL ë‹¨ê³„ì—ì„œ)\n","    if train:\n","        if aug_mode == \"weak\":\n","            ops += [\n","                transforms.RandomHorizontalFlip(0.5),\n","                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","            ]\n","\n","        elif aug_mode in [\"baseline\", \"medium\"]:\n","            ops += [\n","                transforms.RandomHorizontalFlip(0.5),\n","                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","            ]\n","\n","        elif aug_mode == \"strong\":\n","            ops += [\n","                transforms.RandomHorizontalFlip(0.5),\n","                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n","            ]\n","        elif aug_mode == \"super_strong\":\n","            ops += [\n","                transforms.RandomHorizontalFlip(0.5),\n","                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n","            ]\n","\n","    # Tensor ë³€í™˜ + ì •ê·œí™”\n","    ops.append(transforms.ToTensor())\n","    ops.append(transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225],\n","    ))\n","\n","    # RandomErasing\n","    if train:\n","        if aug_mode == \"strong\":\n","            ops.append(transforms.RandomErasing(p=0.25, scale=(0.02, 0.4), value='random'))\n","        elif aug_mode == \"super_strong\":\n","            ops.append(transforms.RandomErasing(p=0.4, scale=(0.02, 0.4), value='random'))\n","\n","    return transforms.Compose(ops)"],"metadata":{"id":"naUoUVw9yYCj","executionInfo":{"status":"ok","timestamp":1765133254927,"user_tz":-540,"elapsed":5,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8x5cLRLUJmLE","executionInfo":{"status":"ok","timestamp":1765133254931,"user_tz":-540,"elapsed":3,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["# -------------------\n","# Dataset\n","# -------------------\n","class DeepFashionC2S(torch.utils.data.Dataset):\n","    def __init__(self, csv_path, img_root, transform=None, split='train'):\n","        self.df = pd.read_csv(csv_path)\n","        self.df = self.df[self.df['split']==split].reset_index(drop=True)\n","        self.img_root = img_root\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def load_crop(self, img_path, x1, y1, x2, y2):\n","        full_path = os.path.join(self.img_root, img_path)\n","        if not os.path.exists(full_path):\n","            raise FileNotFoundError(f\"File not found: {full_path}\")\n","        img = Image.open(full_path).convert(\"RGB\")\n","        return img.crop((x1, y1, x2, y2))\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        try:\n","            cons_img = self.load_crop(row['consumer_path'], row['cons_x1'], row['cons_y1'], row['cons_x2'], row['cons_y2'])\n","            shop_img = self.load_crop(row['shop_path'], row['shop_x1'], row['shop_y1'], row['shop_x2'], row['shop_y2'])\n","            if self.transform:\n","                cons_img = self.transform(cons_img)\n","                shop_img = self.transform(shop_img)\n","            return {\"consumer\": cons_img, \"shop\": shop_img, \"item_id\": row[\"item_id\"]}\n","        except FileNotFoundError:\n","            print(f\"[WARNING] File missing at idx {idx}. Skipping.\")\n","            return None"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TU0u3b3c1IV4","executionInfo":{"status":"ok","timestamp":1765133254943,"user_tz":-540,"elapsed":6,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["# ========================================================\n","# 2. ëª¨ë¸ ì •ì˜\n","# ========================================================\n","\n","# ë°±ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (Feature Embedding)\n","class FeatureEmbedding(nn.Module):\n","    def __init__(self, backbone_name, loss_name, embedding_dim):\n","        super().__init__()\n","\n","        # 1. ë°±ë³¸ ë¡œë“œ ë° ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì„¤ì •\n","        if backbone_name == 'ResNet-50':\n","            base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","            num_ftrs = base_model.fc.in_features\n","\n","        elif backbone_name == 'ResNet-34':\n","            base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n","            num_ftrs = base_model.fc.in_features\n","\n","        elif backbone_name == 'EfficientNet-B3':\n","            base_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n","            num_ftrs = base_model.classifier[1].in_features\n","\n","        else:\n","            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n","\n","        # 2. íŠ¹ì§• ì¶”ì¶œê¸° (ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±°)\n","        if 'ResNet' in backbone_name:\n","            self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n","            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n","        elif 'EfficientNet' in backbone_name:\n","            self.feature_extractor = base_model.features\n","            self.avgpool = nn.AdaptiveAvgPool2d(1)\n","            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n","\n","        self.bn = nn.BatchNorm1d(embedding_dim)\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        if hasattr(self, 'avgpool'):\n","            x = self.avgpool(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = self.final_fc(x)\n","        x = self.bn(x)\n","        x = F.normalize(x, p=2, dim=1)\n","        return x.to(DEVICE)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"KagI8InF1Rfv","executionInfo":{"status":"ok","timestamp":1765133255007,"user_tz":-540,"elapsed":57,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["# ========================================================\n","# 3. ì†ì‹¤í•¨ìˆ˜ ì •ì˜\n","# ========================================================\n","\n","def pairwise_distance_sq(embeddings: torch.Tensor) -> torch.Tensor:\n","    \"\"\"ë°°ì¹˜ ë‚´ ëª¨ë“  ì„ë² ë”© ìŒ ì‚¬ì´ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ì œê³±ì„ ê³„ì‚°.\"\"\"\n","    dot_product = torch.matmul(embeddings, embeddings.t())\n","    square_norm = torch.diag(dot_product)\n","    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n","    distances[distances < 0] = 0\n","    return distances\n","\n","# Online Semi-Hard Triplet Loss (Semi-Hard Mining êµ¬í˜„)\n","def online_semi_hard_triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float) -> torch.Tensor:\n","    \"\"\"\n","    Online Semi-Hard Triplet Loss: ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ë§Œ ê°€ì¥ ì–´ë µì§€ëŠ” ì•Šì€ Negativeë¥¼ ì„ íƒí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ.\n","    \"\"\"\n","    pair_dist = pairwise_distance_sq(embeddings)\n","    labels = labels.to(pair_dist.device)\n","    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n","\n","    # Hard Positive (A-P ì¤‘ ê°€ì¥ ë¨¼ ê±°ë¦¬)\n","    dist_ap = pair_dist.clone()\n","    dist_ap[~labels_eq] = float('-inf')\n","    hard_positive_dist, _ = dist_ap.max(dim=1)\n","\n","    # Semi-Hard Negative (D(A,N) > D(A,P) ì´ë©´ì„œ ê°€ì¥ ê°€ê¹Œìš´ N)\n","    dist_an = pair_dist.clone()\n","    dist_an[labels_eq] = float('inf')\n","\n","    # Semi-Hard ì¡°ê±´: D(A,N) > D(A,P) + marginì„ ë§Œì¡±í•˜ëŠ” ì˜ì—­\n","    # ë§ˆì§„ ì¡°ê±´ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” (A-P ê±°ë¦¬ê°€ A-Në³´ë‹¤ ì´ë¯¸ margin ì´ìƒ ì‘ì€) Nì„ ì°¾ê¸° ìœ„í•´ ë¶€ë“±í˜¸ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n","    # Triplet Loss: D(A,P) - D(A,N) + margin > 0 ì¸ íŠ¸ë¦½ë ›ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n","    # Semi-HardëŠ” D(A,P) < D(A,N) < D(A,P) + margin ì…ë‹ˆë‹¤.\n","\n","    # 1. Negative ì¤‘ ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” í›„ë³´ (D(A,N) > D(A,P))\n","    is_semi_hard_candidate = dist_an > hard_positive_dist.unsqueeze(1)\n","\n","    # 2. Semi-Hard Negative ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ\n","    # D(A,N)ì´ D(A,P)ë³´ë‹¤ í¬ë©´ì„œ, A-P ë§ˆì§„(alpha)ì„ ë„˜ì§€ ì•ŠëŠ” ì˜ì—­ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ Nì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n","    # PyTorch ë‚´ì¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë³µì¡í•œ ì¸ë±ì‹± ëŒ€ì‹  ì•ˆì •ì„±ì„ ìœ„í•´ D(A,N) > D(A,P)ì¸ ìƒ˜í”Œë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n","    dist_an[~is_semi_hard_candidate] = float('inf')\n","    semi_hard_negative_dist, _ = dist_an.min(dim=1)\n","\n","    # Triplet Loss ê³„ì‚°: L = max(0, D(A, P)_hard - D(A, N)_semi_hard + margin)\n","    triplet_loss = hard_positive_dist - semi_hard_negative_dist + margin\n","    triplet_loss[triplet_loss < 0] = 0\n","\n","    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n","\n","    # 8ì—í­ ì˜¤ë¥˜ í›„ ì—¬ê¸° ìˆ˜ì •í•¨\n","    if num_hard_triplets > 0:\n","        return triplet_loss.sum() / num_hard_triplets\n","    else:\n","        # ì—¬ê¸°ê°€ í•µì‹¬: ê·¸ë˜í”„ì— ë¶™ì–´ìˆëŠ” \"0\"ì„ ë¦¬í„´\n","        # embeddings ì—ì„œ ë§Œë“¤ì–´ì§„ ê°’ì´ë¯€ë¡œ requires_grad=True ìœ ì§€ë¨\n","        return (embeddings * 0.0).sum()\n","\n","# Batch-Hard Triplet Loss\n","def batch_hard_triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float) -> torch.Tensor:\n","    \"\"\"Batch-Hard Triplet Loss\"\"\"\n","    pair_dist = pairwise_distance_sq(embeddings)\n","    labels = labels.to(pair_dist.device)\n","    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n","\n","    # hardest positive\n","    dist_ap = pair_dist.clone()\n","    dist_ap[~labels_eq] = float('-inf')\n","    hardest_positive_dist, _ = dist_ap.max(dim=1)\n","\n","    # hardest negative\n","    dist_an = pair_dist.clone()\n","    dist_an[labels_eq] = float('inf')\n","    hardest_negative_dist, _ = dist_an.min(dim=1)\n","\n","    triplet_loss = hardest_positive_dist - hardest_negative_dist + margin\n","    triplet_loss = torch.clamp(triplet_loss, min=0.0)\n","\n","    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n","    if num_hard_triplets > 0:\n","        return triplet_loss.sum() / num_hard_triplets\n","    else:\n","        return (embeddings * 0.0).sum()\n","\n","# InfoNCE Loss\n","def info_nce_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float = 0.07) -> torch.Tensor:\n","    \"\"\"\n","    InfoNCE Loss (Contrastive)\n","    margin ëŒ€ì‹  temperatureë¥¼ margin ì¸ìë¡œ ì¬í™œìš©\n","    \"\"\"\n","    temperature = margin\n","    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n","\n","    # ìê¸° ìì‹  ì œì™¸\n","    mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=embeddings.device)\n","    sim_matrix.masked_fill_(mask, -9e15)\n","\n","    labels = labels.contiguous().view(-1, 1)\n","    matches = torch.eq(labels, labels.T).float() # ê°™ì€ ë¼ë²¨ì´ë©´ 1, ì•„ë‹ˆë©´ 0\n","\n","    exp_sim = torch.exp(sim_matrix)\n","    pos_sim = (exp_sim * matches).sum(dim=1)\n","    all_sim = exp_sim.sum(dim=1)\n","\n","    loss = -torch.log(pos_sim / (all_sim + 1e-9))\n","    return loss.mean()"]},{"cell_type":"code","source":["# ========================================================\n","# 4. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ë° í•™ìŠµ ë£¨í”„\n","# ========================================================\n","\n","def get_checkpoint_paths(backbone_name: str, loss_name: str, mode_name: str):\n","    \"\"\"\n","    backbone_name ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ì™€ ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ê²½ë¡œë¥¼ í•œ ë²ˆì— ìƒì„±.\n","    ì˜ˆ) ResNet-34, EfficientNet-B3 ë“±\n","    \"\"\"\n","    # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ (ë§ˆì§€ë§‰ epoch ê¸°ì¤€)\n","    checkpoint_file = os.path.join(\n","        CHECKPOINT_DIR,\n","        f\"{backbone_name}_{loss_name}_{mode_name}_checkpoint.pth\"\n","    )\n","\n","    # ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ (ìµœê³  val R@5 ê¸°ì¤€)\n","    best_file = os.path.join(\n","        CHECKPOINT_DIR,\n","        f\"{backbone_name}_{loss_name}_{mode_name}_best_weights.pth\"\n","    )\n","\n","    return checkpoint_file, best_file\n","\n","\n","def save_checkpoint(model, optimizer, epoch, best_val_metric, patience_count, filename):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'best_val_metric': best_val_metric,  # Recall@K ê°’ (í´ìˆ˜ë¡ ì¢‹ìŒ)\n","        'patience_count': patience_count,\n","    }\n","\n","    # ìƒìœ„ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ì£¼ê¸° (ì•ˆì „ì¥ì¹˜)\n","    os.makedirs(os.path.dirname(filename), exist_ok=True)\n","\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(model, optimizer, filename):\n","    if not os.path.exists(filename):\n","        print(\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n","        # Recall@KëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ, best_val_metricì„ 0.0ìœ¼ë¡œ ì´ˆê¸°í™”\n","        return 0, 0.0, 0\n","\n","    checkpoint = torch.load(filename, map_location=DEVICE)\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    start_epoch = checkpoint['epoch'] + 1\n","    best_val_metric = checkpoint['best_val_metric']\n","    patience_counter = checkpoint['patience_count']\n","\n","    print(\n","        f\"âœ… í•™ìŠµ ì¬ê°œ: Epoch {start_epoch}ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. \"\n","        f\"(ìµœê³  Recall@5: {best_val_metric:.4f})\"\n","    )\n","\n","    return start_epoch, best_val_metric, patience_counter\n"],"metadata":{"id":"-13nwbmMQ0vv","executionInfo":{"status":"ok","timestamp":1765133255009,"user_tz":-540,"elapsed":1,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"jV_InoDJpujk","executionInfo":{"status":"ok","timestamp":1765133255115,"user_tz":-540,"elapsed":105,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["def train_model(backbone_name, loss_name, mode_name, train_dl, val_dl, criterion):\n","\n","     # --- ì´ˆê¸°í™”/ë³µêµ¬ ---\n","    model = FeatureEmbedding(backbone_name, loss_name, EMBEDDING_DIM).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    # ì²´í¬í¬ì¸íŠ¸ / ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ í•œ ë²ˆì— ì„¤ì •\n","    checkpoint_file, best_file = get_checkpoint_paths(backbone_name, loss_name, mode_name)\n","\n","    # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ\n","    start_epoch, best_val_metric, patience_counter = load_checkpoint(\n","        model, optimizer, checkpoint_file\n","    )\n","\n","    # ì‹¤í—˜ ê¸°ë¡ ë³µêµ¬\n","    history_file = os.path.join(CHECKPOINT_DIR, f\"{backbone_name}_{loss_name}_{mode_name}_history.csv\")\n","    history = []\n","\n","    if os.path.exists(history_file) and os.path.getsize(history_file) > 0:\n","        try:\n","            history_df = pd.read_csv(history_file)\n","            if not history_df.empty:\n","                history = history_df.to_dict('records')\n","            else:\n","                raise pd.errors.EmptyDataError(\"DataFrame is empty after reading.\")\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"âš ï¸ {backbone_name}_{loss_name}_{mode_name}history.csv íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n","            history = []\n","        except Exception as e:\n","            print(f\"âš ï¸ History file loading error ({e.__class__.__name__}): {e}. Starting new history.\")\n","            history = []\n","\n","    # --- í•™ìŠµ ë£¨í”„ ---\n","    for epoch in range(start_epoch, MAX_EPOCHS):\n","\n","        print(f\"\\n--- Starting Epoch {epoch + 1}/{MAX_EPOCHS} for {backbone_name} + {loss_name} + {mode_name} ---\")\n","\n","        model.train()\n","        total_loss = 0\n","\n","        # Batch-All Triplet Lossë¥¼ ìœ„í•œ ë°ì´í„° ë¡œë”© ë£¨í”„\n","        for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1} Train ({backbone_name}) + {loss_name} + {mode_name}\", leave=False):\n","            consumer_imgs = batch[\"consumer\"].to(DEVICE)\n","            shop_imgs = batch[\"shop\"].to(DEVICE)\n","            item_ids = batch[\"item_id\"]\n","\n","            # ë¬¸ìì—´ item_id â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜\n","            if isinstance(item_ids, (list, tuple)):\n","                item_ids = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                item_ids = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","\n","            item_ids = item_ids.to(DEVICE)\n","\n","            # ì´ë¯¸ì§€/ë¼ë²¨ ë³‘í•©\n","            all_imgs = torch.cat([consumer_imgs, shop_imgs], dim=0)\n","            all_labels = torch.cat([item_ids, item_ids], dim=0)\n","\n","            # ëª¨ë¸ ì—…ë°ì´íŠ¸\n","            optimizer.zero_grad()\n","            embeddings = model(all_imgs)\n","\n","            if loss_name == \"infoNce\":\n","                loss = criterion(embeddings, all_labels, margin=0.07)  # temperature = 0.07\n","            else:\n","                loss = criterion(embeddings, all_labels, TRIPLET_MARGIN)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_train_loss = total_loss / len(train_dl)\n","\n","        # --- ê²€ì¦ ë£¨í”„: ë§¤ ì—í¬í¬ë§ˆë‹¤ ì‹¤í–‰ ---\n","        val_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1, 5, 10])\n","        val_metric = val_recalls['R@5']\n","\n","        print(\n","            f\"Epoch {epoch+1} | \"\n","            f\"Train Loss: {avg_train_loss:.4f} | \"\n","            f\"R@1: {val_recalls['R@1']:.3f} | \"\n","            f\"R@5: {val_recalls['R@5']:.3f} | \"\n","            f\"R@10: {val_recalls['R@10']:.3f}\"\n","        )\n","\n","        history.append({\n","            'epoch': epoch + 1,\n","            'train_loss': avg_train_loss,\n","            'val_R@1': val_recalls['R@1'],\n","            'val_R@5': val_recalls['R@5'],\n","            'val_R@10': val_recalls['R@10']\n","        })\n","\n","        # --- Early Stopping ---\n","        if val_metric > best_val_metric:\n","            best_val_metric = val_metric\n","            patience_counter = 0\n","\n","            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n","            save_checkpoint(\n","                model, optimizer, epoch, best_val_metric, patience_counter,\n","                best_file\n","            )\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                print(f\"Early stopping triggered at epoch {epoch+1}.\")\n","                pd.DataFrame(history).to_csv(history_file, index=False)\n","                break\n","\n","        # Epoch ì¢…ë£Œ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n","        save_checkpoint(model, optimizer, epoch, best_val_metric, patience_counter, checkpoint_file)\n","\n","        # history ì €ì¥\n","        pd.DataFrame(history).to_csv(history_file, index=False)\n","\n","    # --- ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---\n","    try:\n","        # ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ\n","        load_checkpoint(model, optimizer, best_file)\n","\n","        # ìµœì¢… Best Recall@K ê³„ì‚°\n","        final_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1, 5, 10])\n","\n","        print(\"\\n========================================================\")\n","        print(f\"ğŸ† {backbone_name} + {loss_name} + {mode_name} Final Best R@1: {final_recalls['R@1']:.4f}\")\n","        print(f\"ğŸ† {backbone_name} + {loss_name} + {mode_name} Final Best R@5: {final_recalls['R@5']:.4f}\")\n","        print(f\"ğŸ† {backbone_name} + {loss_name} + {mode_name} Final Best R@10: {final_recalls['R@10']:.4f}\")\n","        print(\"========================================================\\n\")\n","\n","        return {\"backbone\": backbone_name, \"loss\": loss_name, \"final_epoch\": epoch + 1, \"best_R@5\": final_recalls['R@5']}\n","\n","    except Exception as e:\n","        print(f\"Final evaluation failed: {e}\")\n","        return {\"backbone\": backbone_name, \"loss\": loss_name, \"final_epoch\": epoch + 1, \"best_R@5\": 0.0}"]},{"cell_type":"code","source":["# ========================================================\n","# calculate_recall_at_k í•¨ìˆ˜ (Dict Handling & Numeric Conversion)\n","# ========================================================\n","def calculate_recall_at_k(model, dataloader, device, ks=[1, 5, 10]):\n","    \"\"\"\n","    Validation Setì˜ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³  Recall@K ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n","    (DataLoaderê°€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ê³ , item_idë¥¼ ìˆ«ì ë¼ë²¨ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©)\n","    \"\"\"\n","    model.eval()\n","    all_query_embs = []\n","    all_gallery_embs = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=f\"Calculating Recall@{ks[-1]}\"):\n","\n","            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ batchì—ì„œ ë°ì´í„° ì¶”ì¶œ\n","            consumer_imgs = batch[\"consumer\"]\n","            shop_imgs = batch[\"shop\"]\n","            item_ids = batch[\"item_id\"] # ë¬¸ìì—´ (str) ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŠœí”Œ\n","\n","            # ë¬¸ìì—´ item_id â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ (train_modelê³¼ ë™ì¼í•œ ë¡œì§ ì ìš©)\n","            if isinstance(item_ids, list) or isinstance(item_ids, tuple):\n","                # Global id2label ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ê°€ì •\n","                item_ids_tensor = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                item_ids_tensor = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","\n","            # ì„ë² ë”© ê³„ì‚°\n","            query_embs = model(consumer_imgs.to(device)).cpu().numpy()\n","            gallery_embs = model(shop_imgs.to(device)).cpu().numpy()\n","\n","            all_query_embs.append(query_embs)\n","            all_gallery_embs.append(gallery_embs)\n","            all_labels.append(item_ids_tensor.cpu().numpy()) # ìˆ«ì Tensorë¥¼ NumPy ë°°ì—´ë¡œ ì €ì¥\n","\n","    query_embs = np.concatenate(all_query_embs, axis=0)\n","    gallery_embs = np.concatenate(all_gallery_embs, axis=0)\n","    gallery_labels = np.concatenate(all_labels, axis=0) # [N,] í˜•íƒœì˜ ìˆ«ì ë°°ì—´\n","\n","    # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ í–‰ë ¬)\n","    sims = cosine_similarity(query_embs, gallery_embs)\n","\n","    recalls = {}\n","    for k in ks:\n","        # ìƒìœ„ Kê°œì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ (ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ)\n","        topk_idx = np.argsort(-sims, axis=1)[:, :k]\n","\n","        correct_count = 0\n","        for i in range(len(gallery_embs)):\n","            query_true_id = gallery_labels[i]\n","\n","            # ìƒìœ„ Kê°œì˜ ê°¤ëŸ¬ë¦¬ ìƒí’ˆ ID\n","            topk_ids = gallery_labels[topk_idx[i]]\n","\n","            # ì •ë‹µ IDê°€ ìƒìœ„ Kê°œ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ìˆ«ì ë¹„êµ)\n","            if query_true_id in topk_ids:\n","                correct_count += 1\n","\n","        recalls[f'R@{k}'] = correct_count / len(gallery_labels)\n","\n","    return recalls"],"metadata":{"id":"_wb7pYAhQ6fx","executionInfo":{"status":"ok","timestamp":1765133255117,"user_tz":-540,"elapsed":1,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"_xT7M_TDMk87","executionInfo":{"status":"ok","timestamp":1765133255119,"user_tz":-540,"elapsed":1,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["def custom_collate_fn(batch: List[Dict]):\n","    \"\"\"Noneì„ ë°˜í™˜í•˜ëŠ” ìƒ˜í”Œ(íŒŒì¼ ëˆ„ë½)ì„ í•„í„°ë§í•˜ê³  collate.\"\"\"\n","    # Noneì¸ ìƒ˜í”Œì„ í•„í„°ë§í•©ë‹ˆë‹¤.\n","    batch = [item for item in batch if item is not None]\n","\n","    # ë°°ì¹˜ì— ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ None ë°˜í™˜\n","    if not batch:\n","        return None\n","\n","    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ collate\n","    return default_collate(batch)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"z_xd208wa2DJ","executionInfo":{"status":"ok","timestamp":1765133255121,"user_tz":-540,"elapsed":1,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"outputs":[],"source":["NUM_WORKERS = 4"]},{"cell_type":"code","source":["# ========================================================\n","# ì‹¤í—˜ ì‹¤í–‰ (ì„¸ ê°€ì§€ ì¦ê°• ëª¨ë“œ ìˆœì°¨ ì§„í–‰)\n","# ========================================================\n","if __name__ == '__main__':\n","\n","    # ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸\n","    BACKBONES_TO_TEST = ['EfficientNet-B3']\n","    LOSS_FUNCTIONS_TO_TEST = [(\"batchHardTriplet\", batch_hard_triplet_loss)]\n","    AUGMENT_MODES = [\n","        # \"weak\",\n","        \"medium\",\n","        # \"strong\",\n","        # \"super_strong\"\n","        \"baseline\",\n","    ]\n","\n","    all_results = []\n","\n","    for backbone in BACKBONES_TO_TEST:\n","        for loss_name, criterion in LOSS_FUNCTIONS_TO_TEST:\n","            for mode_name in AUGMENT_MODES:\n","\n","              print(f\"\\n================ Running Experiment: {backbone} + {loss_name} + Aug_{mode_name} ================\")\n","\n","              train_transform = build_transforms(aug_mode=mode_name, train=True)\n","              val_transform = build_transforms(aug_mode=mode_name, train=False)\n","\n","              train_ds = DeepFashionC2S(\n","                  csv_path=CSV_PATH_LIGHT,\n","                  img_root=IMG_ROOT_DIR,\n","                  transform=train_transform,\n","                  split='train'\n","              )\n","              val_ds = DeepFashionC2S(\n","                  csv_path=CSV_PATH_LIGHT,\n","                  img_root=IMG_ROOT_DIR,\n","                  transform=val_transform,\n","                  split='val'\n","              )\n","\n","              train_dl = DataLoader(\n","                  train_ds,\n","                  batch_size=BATCH_SIZE,\n","                  shuffle=True,\n","                  num_workers=NUM_WORKERS,\n","                  generator=generator,\n","                  pin_memory=True,\n","                  collate_fn=custom_collate_fn\n","              )\n","\n","              val_dl = DataLoader(\n","                  val_ds,\n","                  batch_size=BATCH_SIZE,\n","                  shuffle=False,\n","                  num_workers=NUM_WORKERS,\n","                  pin_memory=True,\n","                  collate_fn=custom_collate_fn\n","              )\n","\n","              results = train_model(backbone, loss_name, mode_name, train_dl, val_dl, criterion)\n","              all_results.append(results)\n","\n","    print(\"\\n========== ALL EXPERIMENTS FINAL SUMMARY ==========\")\n","    print(pd.DataFrame(all_results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5atMihPURP9p","outputId":"a80c86b7-c4ec-40b6-8935-49a3d7c93fb5","executionInfo":{"status":"ok","timestamp":1765136667823,"user_tz":-540,"elapsed":3412695,"user":{"displayName":"ê¹€ë¯¼ì§€","userId":"17923879410127250407"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ Running Experiment: EfficientNet-B3 + batchHardTriplet + Aug_medium ================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.2M/47.2M [00:00<00:00, 201MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… í•™ìŠµ ì¬ê°œ: Epoch 14ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. (ìµœê³  Recall@5: 0.6609)\n","\n","--- Starting Epoch 15/40 for EfficientNet-B3 + batchHardTriplet + medium ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 15 Train (EfficientNet-B3) + batchHardTriplet + medium:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15 | Train Loss: 0.1465 | R@1: 0.591 | R@5: 0.644 | R@10: 0.677\n","Early stopping triggered at epoch 15.\n","âœ… í•™ìŠµ ì¬ê°œ: Epoch 10ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. (ìµœê³  Recall@5: 0.6609)\n"]},{"output_type":"stream","name":"stderr","text":["\rCalculating Recall@10:   0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================================\n","ğŸ† EfficientNet-B3 + batchHardTriplet + medium Final Best R@1: 0.5816\n","ğŸ† EfficientNet-B3 + batchHardTriplet + medium Final Best R@5: 0.6609\n","ğŸ† EfficientNet-B3 + batchHardTriplet + medium Final Best R@10: 0.7037\n","========================================================\n","\n","\n","================ Running Experiment: EfficientNet-B3 + batchHardTriplet + Aug_baseline ================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["âœ… í•™ìŠµ ì¬ê°œ: Epoch 6ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. (ìµœê³  Recall@5: 0.6600)\n","\n","--- Starting Epoch 7/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 7 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 | Train Loss: 0.2050 | R@1: 0.573 | R@5: 0.636 | R@10: 0.677\n","\n","--- Starting Epoch 8/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 8 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 | Train Loss: 0.1961 | R@1: 0.588 | R@5: 0.663 | R@10: 0.711\n","\n","--- Starting Epoch 9/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 9 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 | Train Loss: 0.1784 | R@1: 0.570 | R@5: 0.668 | R@10: 0.709\n","\n","--- Starting Epoch 10/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 10 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Train Loss: 0.1646 | R@1: 0.572 | R@5: 0.647 | R@10: 0.703\n","\n","--- Starting Epoch 11/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 11 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11 | Train Loss: 0.1595 | R@1: 0.580 | R@5: 0.650 | R@10: 0.706\n","\n","--- Starting Epoch 12/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 12 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12 | Train Loss: 0.1561 | R@1: 0.573 | R@5: 0.644 | R@10: 0.706\n","\n","--- Starting Epoch 13/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 13 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13 | Train Loss: 0.1512 | R@1: 0.613 | R@5: 0.678 | R@10: 0.703\n","\n","--- Starting Epoch 14/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 14 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14 | Train Loss: 0.1412 | R@1: 0.592 | R@5: 0.674 | R@10: 0.735\n","\n","--- Starting Epoch 15/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 15 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15 | Train Loss: 0.1423 | R@1: 0.589 | R@5: 0.654 | R@10: 0.703\n","\n","--- Starting Epoch 16/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 16 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16 | Train Loss: 0.1355 | R@1: 0.584 | R@5: 0.670 | R@10: 0.706\n","\n","--- Starting Epoch 17/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 17 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17 | Train Loss: 0.1371 | R@1: 0.577 | R@5: 0.646 | R@10: 0.687\n","\n","--- Starting Epoch 18/40 for EfficientNet-B3 + batchHardTriplet + baseline ---\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 18 Train (EfficientNet-B3) + batchHardTriplet + baseline:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18 | Train Loss: 0.1366 | R@1: 0.616 | R@5: 0.675 | R@10: 0.721\n","Early stopping triggered at epoch 18.\n","âœ… í•™ìŠµ ì¬ê°œ: Epoch 13ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. (ìµœê³  Recall@5: 0.6782)\n"]},{"output_type":"stream","name":"stderr","text":["\rCalculating Recall@10:   0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================================\n","ğŸ† EfficientNet-B3 + batchHardTriplet + baseline Final Best R@1: 0.6126\n","ğŸ† EfficientNet-B3 + batchHardTriplet + baseline Final Best R@5: 0.6782\n","ğŸ† EfficientNet-B3 + batchHardTriplet + baseline Final Best R@10: 0.7028\n","========================================================\n","\n","\n","========== ALL EXPERIMENTS FINAL SUMMARY ==========\n","          backbone              loss  final_epoch  best_R@5\n","0  EfficientNet-B3  batchHardTriplet           15  0.660893\n","1  EfficientNet-B3  batchHardTriplet           18  0.678213\n"]}]}]}