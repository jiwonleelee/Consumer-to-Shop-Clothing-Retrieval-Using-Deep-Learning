{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P31ZlvuDxSzY"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# 1. ğŸ› ï¸ í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ, Import (Code Block 1/2)\n",
        "# ========================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.models as models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Dict, List\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from typing import List, Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ACpB338K0748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268a3acd-26f3-47b9-bc1b-7f9fb257b196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ë§ˆìš´íŠ¸ (ê³µìœ  ë“œë¼ì´ë¸Œ ê²½ë¡œê°€ MyDriveì— ë°”ë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- ê²½ë¡œ ì„¤ì • ---\n",
        "# ì´ ê²½ë¡œê°€ dataset.py, transforms.py, checkpoints í´ë”ê°€ ìˆëŠ” ìœ„ì¹˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
        "MODULE_PATH = \"/content/drive/MyDrive/2025CV\"\n",
        "sys.path.append(MODULE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ’¡ Import Custom Modules (dataset.py, transforms.py)\n",
        "# BBox í¬ë¡­ ë¡œì§ê³¼ Transforms ì •ì˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "from dataset_jw_sy import DeepFashionC2S\n",
        "from transforms import train_transform, val_transform"
      ],
      "metadata": {
        "id": "t5XkGI2OcSbo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G_vYkBF11EK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b039b8-9da7-4d02-a67c-8a7d72f10811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# --- Hyperparameters (íŒ€ì› ê°„ í†µì¼ í•„ìˆ˜) ---\n",
        "EXPERIMENT_SEED = 42\n",
        "EMBEDDING_DIM = 128  # 128ë¡œ ê³ ì •\n",
        "LEARNING_RATE = 1e-4\n",
        "TRIPLET_MARGIN = 0.5 # Online Semi-Hard Triplet Loss ë§ˆì§„ ê°’\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 5         # Early Stopping Patience (5 Epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)\n",
        "MAX_EPOCHS = 40      # ìµœëŒ€ í•™ìŠµ Epoch ìˆ˜\n",
        "CHECKPOINT_ROOT = os.path.join(MODULE_PATH, \"checkpoints_B\")\n",
        "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ì¬í˜„ì„± í™•ë³´ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    return torch.Generator().manual_seed(seed)\n",
        "\n",
        "generator = set_seed(EXPERIMENT_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vVz_Qlb51Hqh"
      },
      "outputs": [],
      "source": [
        "# --- CSV íŒŒì¼ ë¡œë“œ (ìƒ˜í”Œë§ëœ CSV ì‚¬ìš©) ---\n",
        "CSV_PATH_LIGHT = os.path.join(MODULE_PATH, \"meta_c2s_10_2_2_sampling_ID.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# --- ê²½ë¡œ ì„¤ì • í™•ì¸ ---\n",
        "\n",
        "DRIVE_IMG_ROOT = os.path.join(MODULE_PATH, \"Images\") # ì›ë³¸ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Drive)\n",
        "LOCAL_IMG_ROOT = \"/content/Images\"                   # íƒ€ê²Ÿ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (Local)"
      ],
      "metadata": {
        "id": "6embU8hmrO-9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì´ê²Œ 40ë¶„ ê±¸ë ¤ìš”\n",
        "# CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ë³µì‚¬ ë° I/O ìµœì í™”\n",
        "# =========================================================\n",
        "print(\"ğŸš€ CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\")\n",
        "\n",
        "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
        "try:\n",
        "    df_light = pd.read_csv(CSV_PATH_LIGHT)\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {CSV_PATH_LIGHT}\")\n",
        "    exit()\n",
        "\n",
        "# 2. í•„ìš”í•œ ëª¨ë“  ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ\n",
        "# consumer_pathì™€ shop_path ì—´ì—ì„œ ìœ ë‹ˆí¬í•œ ê²½ë¡œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "required_paths = pd.concat([df_light['consumer_path'], df_light['shop_path']]).unique()\n",
        "\n",
        "print(f\"ì´ {len(required_paths)}ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# 3. ë¡œì»¬ íƒ€ê²Ÿ í´ë” ìƒì„±\n",
        "os.makedirs(LOCAL_IMG_ROOT, exist_ok=True)\n",
        "\n",
        "# 4. íŒŒì¼ ë³µì‚¬ ë° í´ë” êµ¬ì¡° ìœ ì§€\n",
        "copied_count = 0\n",
        "for relative_path in required_paths:\n",
        "    # ì›ë³¸ íŒŒì¼ ê²½ë¡œ (Drive)\n",
        "    source_file_path = os.path.join(DRIVE_IMG_ROOT, relative_path)\n",
        "\n",
        "    # íƒ€ê²Ÿ íŒŒì¼ ê²½ë¡œ (Local)\n",
        "    target_file_path = os.path.join(LOCAL_IMG_ROOT, relative_path)\n",
        "\n",
        "    # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„± (ì˜ˆ: /content/Images/img/TOPS/Summer_ ì— í•„ìš”í•œ í´ë” ìƒì„±)\n",
        "    target_dir = os.path.dirname(target_file_path)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # íŒŒì¼ ë³µì‚¬\n",
        "    try:\n",
        "        if not os.path.exists(target_file_path):\n",
        "             shutil.copy2(source_file_path, target_file_path)\n",
        "             copied_count += 1\n",
        "    except FileNotFoundError:\n",
        "        # CSVì— ê²½ë¡œê°€ ìˆì§€ë§Œ ì‹¤ì œ Driveì— íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.\n",
        "        print(f\"[ê²½ê³ ] ì›ë³¸ íŒŒì¼ì´ Driveì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€: {source_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ì˜¤ë¥˜] ë³µì‚¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({relative_path}): {e}\")\n",
        "\n",
        "print(f\"âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ {copied_count}ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\")"
      ],
      "metadata": {
        "id": "H_3ldDfXzwfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c385944-2e4a-487a-b434-37d3dd45d4c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ CSV ê¸°ë°˜ ì„ íƒì  ì´ë¯¸ì§€ ë¡œì»¬ ëŸ°íƒ€ì„ ë³µì‚¬ ì‹œì‘...\n",
            "ì´ 10546ê°œì˜ ìœ ë‹ˆí¬í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
            "âœ… ë¡œì»¬ ë³µì‚¬ ì™„ë£Œ. ì´ 10546ê°œì˜ íŒŒì¼ ë³µì‚¬ë¨.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3. IMG_ROOT_DIR ë³€ìˆ˜ë¥¼ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½\n",
        "# =========================================================\n",
        "# ê¸°ì¡´ ê²½ë¡œ ë³€ìˆ˜ë¥¼ ìƒˆë¡œìš´ ë¡œì»¬ ê²½ë¡œë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n",
        "IMG_ROOT_DIR = LOCAL_IMG_ROOT\n",
        "\n",
        "print(f\"ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: {IMG_ROOT_DIR}\")"
      ],
      "metadata": {
        "id": "B1Zd89dDrACq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760c0d50-0d82-4677-b104-518280acdb4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ: /content/Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6YvjqWS-dlG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56aa8d71-dfe7-49ac-a30f-976b7a683005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ unique item_id ê°œìˆ˜: 1467\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# item_id ë¬¸ìì—´ â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ ë§¤í•‘ ìƒì„±\n",
        "# ============================================\n",
        "\n",
        "df_full = pd.read_csv(CSV_PATH_LIGHT)\n",
        "unique_ids = df_full[\"item_id\"].unique()\n",
        "\n",
        "id2label = {id_str: idx for idx, id_str in enumerate(unique_ids)}\n",
        "print(\"ì´ unique item_id ê°œìˆ˜:\", len(id2label))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TU0u3b3c1IV4"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# 2. ëª¨ë¸ ë° ì†ì‹¤í•¨ìˆ˜ ì •ì˜ (Stage 2 í•µì‹¬) (Code Block 2/2)\n",
        "# ========================================================\n",
        "\n",
        "# ë°±ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (Feature Embedding)\n",
        "class FeatureEmbedding(nn.Module):\n",
        "    def __init__(self, backbone_name, loss_name, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ë°±ë³¸ ë¡œë“œ ë° ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
        "        if backbone_name == 'ResNet-50':\n",
        "            base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "            num_ftrs = base_model.fc.in_features\n",
        "\n",
        "        elif backbone_name == 'ResNet-34':\n",
        "            base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "            num_ftrs = base_model.fc.in_features\n",
        "\n",
        "        elif backbone_name == 'EfficientNet-B3':\n",
        "            base_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "            num_ftrs = base_model.classifier[1].in_features\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
        "\n",
        "        # 2. íŠ¹ì§• ì¶”ì¶œê¸° (ë§ˆì§€ë§‰ FC ë ˆì´ì–´ ì œê±°)\n",
        "        if 'ResNet' in backbone_name:\n",
        "            self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
        "            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n",
        "        elif 'EfficientNet' in backbone_name:\n",
        "            self.feature_extractor = base_model.features\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "            self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        if hasattr(self, 'avgpool'):\n",
        "            x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.final_fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KagI8InF1Rfv"
      },
      "outputs": [],
      "source": [
        "# Online Semi-Hard Triplet Loss í•¨ìˆ˜\n",
        "def pairwise_distance_sq(embeddings: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"ë°°ì¹˜ ë‚´ ëª¨ë“  ì„ë² ë”© ìŒ ì‚¬ì´ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ì œê³±ì„ ê³„ì‚°.\"\"\"\n",
        "    dot_product = torch.matmul(embeddings, embeddings.t())\n",
        "    square_norm = torch.diag(dot_product)\n",
        "    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n",
        "    distances[distances < 0] = 0\n",
        "    return distances\n",
        "\n",
        "# ì˜¨ë¼ì¸ ì„¸ë¯¸-í•˜ë“œ íŠ¸ë¦½ë › ì†ì‹¤ í•¨ìˆ˜ (Semi-Hard Mining êµ¬í˜„)\n",
        "def online_semi_hard_triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Online Semi-Hard Triplet Loss: ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ë§Œ ê°€ì¥ ì–´ë µì§€ëŠ” ì•Šì€ Negativeë¥¼ ì„ íƒí•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ.\n",
        "    \"\"\"\n",
        "    pair_dist = pairwise_distance_sq(embeddings)\n",
        "    labels = labels.to(pair_dist.device)\n",
        "    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n",
        "\n",
        "    # Hard Positive (A-P ì¤‘ ê°€ì¥ ë¨¼ ê±°ë¦¬)\n",
        "    dist_ap = pair_dist.clone()\n",
        "    dist_ap[~labels_eq] = float('-inf')\n",
        "    hard_positive_dist, _ = dist_ap.max(dim=1)\n",
        "\n",
        "    # Semi-Hard Negative (D(A,N) > D(A,P) ì´ë©´ì„œ ê°€ì¥ ê°€ê¹Œìš´ N)\n",
        "    dist_an = pair_dist.clone()\n",
        "    dist_an[labels_eq] = float('inf')\n",
        "\n",
        "    # Semi-Hard ì¡°ê±´: D(A,N) > D(A,P) + marginì„ ë§Œì¡±í•˜ëŠ” ì˜ì—­\n",
        "    # ë§ˆì§„ ì¡°ê±´ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” (A-P ê±°ë¦¬ê°€ A-Në³´ë‹¤ ì´ë¯¸ margin ì´ìƒ ì‘ì€) Nì„ ì°¾ê¸° ìœ„í•´ ë¶€ë“±í˜¸ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    # Triplet Loss: D(A,P) - D(A,N) + margin > 0 ì¸ íŠ¸ë¦½ë ›ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
        "    # Semi-HardëŠ” D(A,P) < D(A,N) < D(A,P) + margin ì…ë‹ˆë‹¤.\n",
        "\n",
        "    # 1. Negative ì¤‘ ë§ˆì§„ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” í›„ë³´ (D(A,N) > D(A,P))\n",
        "    is_semi_hard_candidate = dist_an > hard_positive_dist.unsqueeze(1)\n",
        "\n",
        "    # 2. Semi-Hard Negative ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ\n",
        "    # D(A,N)ì´ D(A,P)ë³´ë‹¤ í¬ë©´ì„œ, A-P ë§ˆì§„(alpha)ì„ ë„˜ì§€ ì•ŠëŠ” ì˜ì—­ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ Nì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
        "    # PyTorch ë‚´ì¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë³µì¡í•œ ì¸ë±ì‹± ëŒ€ì‹  ì•ˆì •ì„±ì„ ìœ„í•´ D(A,N) > D(A,P)ì¸ ìƒ˜í”Œë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
        "    dist_an[~is_semi_hard_candidate] = float('inf')\n",
        "    semi_hard_negative_dist, _ = dist_an.min(dim=1)\n",
        "\n",
        "    # Triplet Loss ê³„ì‚°: L = max(0, D(A, P)_hard - D(A, N)_semi_hard + margin)\n",
        "    triplet_loss = hard_positive_dist - semi_hard_negative_dist + margin\n",
        "    triplet_loss[triplet_loss < 0] = 0\n",
        "\n",
        "    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n",
        "\n",
        "    # 8ì—í­ ì˜¤ë¥˜ í›„ ì—¬ê¸° ìˆ˜ì •í•¨\n",
        "    if num_hard_triplets > 0:\n",
        "        return triplet_loss.sum() / num_hard_triplets\n",
        "    else:\n",
        "        # âš ï¸ ì—¬ê¸°ê°€ í•µì‹¬: ê·¸ë˜í”„ì— ë¶™ì–´ìˆëŠ” \"0\"ì„ ë¦¬í„´\n",
        "        # embeddings ì—ì„œ ë§Œë“¤ì–´ì§„ ê°’ì´ë¯€ë¡œ requires_grad=True ìœ ì§€ë¨\n",
        "        return (embeddings * 0.0).sum()\n",
        "\n",
        "# Batch-Hard Triplet Loss\n",
        "def batch_hard_triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float) -> torch.Tensor:\n",
        "    \"\"\"Batch-Hard Triplet Loss\"\"\"\n",
        "    pair_dist = pairwise_distance_sq(embeddings)\n",
        "    labels = labels.to(pair_dist.device)\n",
        "    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n",
        "\n",
        "    # hardest positive\n",
        "    dist_ap = pair_dist.clone()\n",
        "    dist_ap[~labels_eq] = float('-inf')\n",
        "    hardest_positive_dist, _ = dist_ap.max(dim=1)\n",
        "\n",
        "    # hardest negative\n",
        "    dist_an = pair_dist.clone()\n",
        "    dist_an[labels_eq] = float('inf')\n",
        "    hardest_negative_dist, _ = dist_an.min(dim=1)\n",
        "\n",
        "    triplet_loss = hardest_positive_dist - hardest_negative_dist + margin\n",
        "    triplet_loss = torch.clamp(triplet_loss, min=0.0)\n",
        "\n",
        "    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n",
        "    if num_hard_triplets > 0:\n",
        "        return triplet_loss.sum() / num_hard_triplets\n",
        "    else:\n",
        "        return (embeddings * 0.0).sum()\n",
        "\n",
        "# InfoNCE Loss\n",
        "def info_nce_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float = 0.07) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    InfoNCE Loss (Contrastive)\n",
        "    margin ëŒ€ì‹  temperatureë¥¼ margin ì¸ìë¡œ ì¬í™œìš©\n",
        "    \"\"\"\n",
        "    temperature = margin\n",
        "    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
        "\n",
        "    # ìê¸° ìì‹  ì œì™¸\n",
        "    mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=embeddings.device)\n",
        "    sim_matrix.masked_fill_(mask, -9e15)\n",
        "\n",
        "    labels = labels.contiguous().view(-1, 1)\n",
        "    matches = torch.eq(labels, labels.T).float() # ê°™ì€ ë¼ë²¨ì´ë©´ 1, ì•„ë‹ˆë©´ 0\n",
        "\n",
        "    exp_sim = torch.exp(sim_matrix)\n",
        "    pos_sim = (exp_sim * matches).sum(dim=1)\n",
        "    all_sim = exp_sim.sum(dim=1)\n",
        "\n",
        "    loss = -torch.log(pos_sim / (all_sim + 1e-9))\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O0YIpFve10oG"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# 3. ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ë° í•™ìŠµ ë£¨í”„ (ì •ë¦¬ ë²„ì „)\n",
        "# ========================================================\n",
        "\n",
        "def get_checkpoint_paths(backbone, loss_name, margin_ratio):\n",
        "    exp_dir = os.path.join(\n",
        "        CHECKPOINT_ROOT,\n",
        "        f\"{backbone}_{loss_name}_margin{margin_ratio}\"\n",
        "    )\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    ckpt_file = os.path.join(exp_dir, \"checkpoint.pth\")\n",
        "    best_file = os.path.join(exp_dir, \"best.pth\")\n",
        "    history_file = os.path.join(exp_dir, \"history.csv\")\n",
        "\n",
        "    return ckpt_file, best_file, history_file\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, best_val_metric, patience_count, filename):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_metric': best_val_metric,  # Recall@K ê°’ (í´ìˆ˜ë¡ ì¢‹ìŒ)\n",
        "        'patience_count': patience_count,\n",
        "    }\n",
        "\n",
        "    # ìƒìœ„ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ì£¼ê¸° (ì•ˆì „ì¥ì¹˜)\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "        # Recall@KëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ, best_val_metricì„ 0.0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "        return 0, 0.0, 0\n",
        "\n",
        "    checkpoint = torch.load(filename, map_location=DEVICE)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_metric = checkpoint['best_val_metric']\n",
        "    patience_counter = checkpoint['patience_count']\n",
        "\n",
        "    print(\n",
        "        f\"âœ… í•™ìŠµ ì¬ê°œ: Epoch {start_epoch}ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. \"\n",
        "        f\"(ìµœê³  Recall@5: {best_val_metric:.4f})\"\n",
        "    )\n",
        "\n",
        "    return start_epoch, best_val_metric, patience_counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tFFI2p_-C3i3"
      },
      "outputs": [],
      "source": [
        "def train_model(backbone_name, loss_name, train_dl, val_dl, criterion, margin_ratio):\n",
        "\n",
        "    # --- Margin-aware checkpoint paths ---\n",
        "    checkpoint_file, best_file, history_file = get_checkpoint_paths(\n",
        "        backbone_name, loss_name, margin_ratio\n",
        "    )\n",
        "\n",
        "    # ëª¨ë¸ ìƒì„±\n",
        "    model = FeatureEmbedding(backbone_name, loss_name, EMBEDDING_DIM).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # History & early stopping\n",
        "    best_val = 0.0\n",
        "    patience_counter = 0\n",
        "    history = []\n",
        "\n",
        "    print(f\"\\n===== Training {backbone_name} + {loss_name} | margin={margin_ratio} =====\")\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_dl, desc=f\"Train epoch {epoch}\", leave=False):\n",
        "            consumer_imgs = batch[\"consumer\"].to(DEVICE)\n",
        "            shop_imgs = batch[\"shop\"].to(DEVICE)\n",
        "            item_ids = batch[\"item_id\"]\n",
        "\n",
        "            # String â†’ int label ë³€í™˜\n",
        "            labels = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "            all_imgs = torch.cat([consumer_imgs, shop_imgs], dim=0)\n",
        "            all_labels = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            embeddings = model(all_imgs)\n",
        "            loss = criterion(embeddings, all_labels, TRIPLET_MARGIN)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "\n",
        "        # --- Validation ---\n",
        "        val_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1,5,10])\n",
        "        val_R5 = val_recalls[\"R@5\"]\n",
        "\n",
        "        print(f\"Epoch {epoch} | Loss={avg_loss:.4f} | R@5={val_R5:.4f}\")\n",
        "\n",
        "        # ê¸°ë¡ ì €ì¥\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"val_R@1\": val_recalls[\"R@1\"],\n",
        "            \"val_R@5\": val_R5,\n",
        "            \"val_R@10\": val_recalls[\"R@10\"]\n",
        "        })\n",
        "        pd.DataFrame(history).to_csv(history_file, index=False)\n",
        "\n",
        "        # --- Early Stopping ---\n",
        "        if val_R5 > best_val:\n",
        "            best_val = val_R5\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), best_file)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(\"â›” Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # --- Best Model í‰ê°€ ---\n",
        "    model.load_state_dict(torch.load(best_file))\n",
        "    final_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1,5,10])\n",
        "\n",
        "    return {\n",
        "        \"margin_ratio\": margin_ratio,\n",
        "        \"backbone\": backbone_name,\n",
        "        \"loss\": loss_name,\n",
        "        \"best_R@1\": final_recalls[\"R@1\"],\n",
        "        \"best_R@5\": final_recalls[\"R@5\"],\n",
        "        \"best_R@10\": final_recalls[\"R@10\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# calculate_recall_at_k í•¨ìˆ˜ (Dict Handling & Numeric Conversion)\n",
        "# ========================================================\n",
        "def calculate_recall_at_k(model, dataloader, device, ks=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Validation Setì˜ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³  Recall@K ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
        "    (DataLoaderê°€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ê³ , item_idë¥¼ ìˆ«ì ë¼ë²¨ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_query_embs = []\n",
        "    all_gallery_embs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Calculating Recall@{ks[-1]}\"):\n",
        "\n",
        "            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ batchì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
        "            consumer_imgs = batch[\"consumer\"]\n",
        "            shop_imgs = batch[\"shop\"]\n",
        "            item_ids = batch[\"item_id\"] # ë¬¸ìì—´ (str) ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŠœí”Œ\n",
        "\n",
        "            # ë¬¸ìì—´ item_id â†’ ìˆ«ì ë¼ë²¨ ë³€í™˜ (train_modelê³¼ ë™ì¼í•œ ë¡œì§ ì ìš©)\n",
        "            if isinstance(item_ids, list) or isinstance(item_ids, tuple):\n",
        "                # Global id2label ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ê°€ì •\n",
        "                item_ids_tensor = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n",
        "            else:\n",
        "                item_ids_tensor = torch.tensor([id2label[item_ids]], dtype=torch.long)\n",
        "\n",
        "            # ì„ë² ë”© ê³„ì‚°\n",
        "            query_embs = model(consumer_imgs.to(device)).cpu().numpy()\n",
        "            gallery_embs = model(shop_imgs.to(device)).cpu().numpy()\n",
        "\n",
        "            all_query_embs.append(query_embs)\n",
        "            all_gallery_embs.append(gallery_embs)\n",
        "            all_labels.append(item_ids_tensor.cpu().numpy()) # ìˆ«ì Tensorë¥¼ NumPy ë°°ì—´ë¡œ ì €ì¥\n",
        "\n",
        "    query_embs = np.concatenate(all_query_embs, axis=0)\n",
        "    gallery_embs = np.concatenate(all_gallery_embs, axis=0)\n",
        "    gallery_labels = np.concatenate(all_labels, axis=0) # [N,] í˜•íƒœì˜ ìˆ«ì ë°°ì—´\n",
        "\n",
        "    # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ í–‰ë ¬)\n",
        "    sims = cosine_similarity(query_embs, gallery_embs)\n",
        "\n",
        "    recalls = {}\n",
        "    for k in ks:\n",
        "        # ìƒìœ„ Kê°œì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ (ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ)\n",
        "        topk_idx = np.argsort(-sims, axis=1)[:, :k]\n",
        "\n",
        "        correct_count = 0\n",
        "        for i in range(len(gallery_labels)):\n",
        "            query_true_id = gallery_labels[i]\n",
        "\n",
        "            # ìƒìœ„ Kê°œì˜ ê°¤ëŸ¬ë¦¬ ìƒí’ˆ ID\n",
        "            topk_ids = gallery_labels[topk_idx[i]]\n",
        "\n",
        "            # ì •ë‹µ IDê°€ ìƒìœ„ Kê°œ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ìˆ«ì ë¹„êµ)\n",
        "            if query_true_id in topk_ids:\n",
        "                correct_count += 1\n",
        "\n",
        "        recalls[f'R@{k}'] = correct_count / len(gallery_labels)\n",
        "\n",
        "    return recalls"
      ],
      "metadata": {
        "id": "jV_InoDJpujk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch: List[Dict]):\n",
        "    \"\"\"Noneì„ ë°˜í™˜í•˜ëŠ” ìƒ˜í”Œ(íŒŒì¼ ëˆ„ë½)ì„ í•„í„°ë§í•˜ê³  collate.\"\"\"\n",
        "    # Noneì¸ ìƒ˜í”Œì„ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    # ë°°ì¹˜ì— ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ None ë°˜í™˜\n",
        "    if not batch:\n",
        "        return None\n",
        "\n",
        "    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ collate\n",
        "    return default_collate(batch)"
      ],
      "metadata": {
        "id": "TH1WpbZWyS3b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORKERS = 4"
      ],
      "metadata": {
        "id": "8CF1SyCgyhpR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# 4. ğŸ ì‹¤í—˜ ì‹¤í–‰ - margin = 0.1/0.2\n",
        "# ========================================================\n",
        "\n",
        "MARGINS = [0.1, 0.2]\n",
        "BACKBONE = \"EfficientNet-B3\"\n",
        "LOSS_NAME = \"batchHardTriplet\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for margin in MARGINS:\n",
        "    print(f\"\\n================ Running margin={margin} ================\")\n",
        "\n",
        "    train_ds = DeepFashionC2S(\n",
        "        csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR,\n",
        "        transform=train_transform, split=\"train\", margin_ratio=margin\n",
        "    )\n",
        "    val_ds = DeepFashionC2S(\n",
        "        csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR,\n",
        "        transform=val_transform, split=\"val\", margin_ratio=margin\n",
        "    )\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                          generator=generator)\n",
        "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    result = train_model(\n",
        "        backbone_name=BACKBONE,\n",
        "        loss_name=LOSS_NAME,\n",
        "        train_dl=train_dl,\n",
        "        val_dl=val_dl,\n",
        "        criterion=batch_hard_triplet_loss,\n",
        "        margin_ratio=margin\n",
        "    )\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "cboaanAVrGKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5639c25-300b-4f51-86a3-569ba4e0bd64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ Running margin=0.1 ================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.2M/47.2M [00:00<00:00, 198MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Training EfficientNet-B3 + batchHardTriplet | margin=0.1 =====\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss=0.6675 | R@5=0.4941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 2:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.96it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss=0.4613 | R@5=0.5579\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 3:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss=0.3608 | R@5=0.6035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 4:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Loss=0.2993 | R@5=0.6399\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 5:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Loss=0.2527 | R@5=0.6299\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 6:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Loss=0.2272 | R@5=0.6536\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 7:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Loss=0.1993 | R@5=0.6481\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 8:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Loss=0.1831 | R@5=0.6199\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 9:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Loss=0.1781 | R@5=0.6582\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 10:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Loss=0.1697 | R@5=0.6308\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 11:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Loss=0.1607 | R@5=0.6463\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 12:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.90it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Loss=0.1565 | R@5=0.6582\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 13:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | Loss=0.1544 | R@5=0.6563\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 14:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | Loss=0.1511 | R@5=0.6244\n",
            "â›” Early stopping triggered!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Recall@10:   0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.52it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ Running margin=0.2 ================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Training EfficientNet-B3 + batchHardTriplet | margin=0.2 =====\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.88it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss=0.6705 | R@5=0.4448\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 2:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  3.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss=0.4663 | R@5=0.5561\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 3:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.92it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss=0.3645 | R@5=0.5661\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 4:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.99it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Loss=0.3009 | R@5=0.5871\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 5:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Loss=0.2575 | R@5=0.6135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 6:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Loss=0.2249 | R@5=0.6007\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 7:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.00it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Loss=0.2053 | R@5=0.6180\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 8:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Loss=0.1879 | R@5=0.6044\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 9:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.99it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Loss=0.1774 | R@5=0.6217\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 10:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Loss=0.1705 | R@5=0.6244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 11:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Loss=0.1545 | R@5=0.6180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 12:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Loss=0.1617 | R@5=0.6244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 13:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Loss=0.1502 | R@5=0.6263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 14:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Loss=0.1478 | R@5=0.6354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 15:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Loss=0.1448 | R@5=0.6554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 16:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Loss=0.1461 | R@5=0.6171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 17:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Loss=0.1356 | R@5=0.6235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 18:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Loss=0.1375 | R@5=0.6071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 19:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Loss=0.1249 | R@5=0.6335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain epoch 20:   0%|          | 0/389 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Loss=0.1267 | R@5=0.6217\n",
            "â›” Early stopping triggered!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCalculating Recall@10:   0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Calculating Recall@10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   margin_ratio         backbone              loss  best_R@1  best_R@5  \\\n",
            "0           0.1  EfficientNet-B3  batchHardTriplet  0.583409  0.658159   \n",
            "1           0.2  EfficientNet-B3  batchHardTriplet  0.587056  0.655424   \n",
            "\n",
            "   best_R@10  \n",
            "0   0.707384  \n",
            "1   0.695533  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sntQzOGAr4BN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}