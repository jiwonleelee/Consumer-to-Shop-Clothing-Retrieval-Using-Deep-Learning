{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"P31ZlvuDxSzY","executionInfo":{"status":"ok","timestamp":1765094525622,"user_tz":-540,"elapsed":16859,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# 1. 환경 설정 및 경로, Import\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","import torchvision.models as models\n","import pandas as pd\n","import numpy as np\n","import random\n","import os\n","import sys\n","from tqdm import tqdm\n","from PIL import Image\n","from typing import Tuple, Dict, List\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity\n","from torch.utils.data.dataloader import default_collate\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ACpB338K0748","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765094543005,"user_tz":-540,"elapsed":17377,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}},"outputId":"d784163e-3d0b-4195-c226-36f9cdc374c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 마운트 (공유 드라이브 경로가 MyDrive에 바로 연결되어 있다고 가정)\n","drive.mount('/content/drive')\n","\n","# --- 경로 설정 ---\n","# 이 경로가 dataset.py, transforms.py, checkpoints 폴더가 있는 위치여야 합니다.\n","MODULE_PATH = \"/content/drive/MyDrive/2025CV\"\n","sys.path.append(MODULE_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"t5XkGI2OcSbo","executionInfo":{"status":"ok","timestamp":1765094544712,"user_tz":-540,"elapsed":1708,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# Import Custom Modules (dataset.py, transforms.py)\n","# BBox 크롭 로직과 Transforms 정의를 가져옵니다.\n","from dataset_jw import DeepFashionC2S\n","from transforms import train_transform, val_transform"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1765094544745,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"},"user_tz":-540},"id":"G_vYkBF11EK4","outputId":"77487670-1145-4544-e595-5ab746403711"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["# --- Hyperparameters (팀원 간 통일 필수) ---\n","EXPERIMENT_SEED = 42\n","EMBEDDING_DIM = 128  # 128로 고정\n","LEARNING_RATE = 1e-4\n","TRIPLET_MARGIN = 0.5 # Online Semi-Hard Triplet Loss 마진 값\n","BATCH_SIZE = 32\n","PATIENCE = 5         # Early Stopping Patience (5 Epoch 동안 개선 없으면 중단)\n","MAX_EPOCHS = 40      # 최대 학습 Epoch 수\n","CHECKPOINT_DIR = os.path.join(MODULE_PATH, \"checkpoints_C\")\n","\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# 재현성 확보를 위한 시드 고정\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    return torch.Generator().manual_seed(seed)\n","\n","generator = set_seed(EXPERIMENT_SEED)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vVz_Qlb51Hqh","executionInfo":{"status":"ok","timestamp":1765094544748,"user_tz":-540,"elapsed":2,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# --- CSV 파일 로드 (샘플링된 CSV 사용) ---\n","CSV_PATH_LIGHT = os.path.join(MODULE_PATH, \"meta_c2s_10_2_2_sampling_ID.csv\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6embU8hmrO-9","executionInfo":{"status":"ok","timestamp":1765094544758,"user_tz":-540,"elapsed":6,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["import os\n","import shutil\n","import pandas as pd\n","\n","# --- 경로 설정 확인 ---\n","\n","DRIVE_IMG_ROOT = os.path.join(MODULE_PATH, \"Images\") # 원본 이미지 루트 경로 (Drive)\n","LOCAL_IMG_ROOT = \"/content/Images\"                   # 타겟 이미지 루트 경로 (Local)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_3ldDfXzwfx","executionInfo":{"status":"ok","timestamp":1765098486856,"user_tz":-540,"elapsed":3942097,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}},"outputId":"2fbb2ff6-1577-48e5-a313-4230b9df0a2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["CSV 기반 선택적 이미지 로컬 런타임 복사 시작...\n","총 10546개의 유니크한 이미지 파일을 복사합니다.\n","✅ 로컬 복사 완료. 총 10546개의 파일 복사됨.\n"]}],"source":["# =========================================================\n","# 이게 40분 걸려요\n","# CSV 기반 선택적 이미지 로컬 복사 및 I/O 최적화\n","# =========================================================\n","print(\"CSV 기반 선택적 이미지 로컬 런타임 복사 시작...\")\n","\n","# 1. CSV 파일 로드\n","try:\n","    df_light = pd.read_csv(CSV_PATH_LIGHT)\n","except FileNotFoundError:\n","    print(f\"오류: CSV 파일을 찾을 수 없습니다. 경로를 확인하세요: {CSV_PATH_LIGHT}\")\n","    exit()\n","\n","# 2. 필요한 모든 이미지 경로 추출\n","# consumer_path와 shop_path 열에서 유니크한 경로만 추출합니다.\n","required_paths = pd.concat([df_light['consumer_path'], df_light['shop_path']]).unique()\n","print(f\"총 {len(required_paths)}개의 유니크한 이미지 파일을 복사합니다.\")\n","\n","# 3. 로컬 타겟 폴더 생성\n","os.makedirs(LOCAL_IMG_ROOT, exist_ok=True)\n","\n","# 4. 파일 복사 및 폴더 구조 유지\n","copied_count = 0\n","for relative_path in required_paths:\n","    # 원본 파일 경로 (Drive)\n","    source_file_path = os.path.join(DRIVE_IMG_ROOT, relative_path)\n","\n","    # 타겟 파일 경로 (Local)\n","    target_file_path = os.path.join(LOCAL_IMG_ROOT, relative_path)\n","\n","    # 타겟 디렉토리 생성 (예: /content/Images/img/TOPS/Summer_ 에 필요한 폴더 생성)\n","    target_dir = os.path.dirname(target_file_path)\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    # 파일 복사\n","    try:\n","        if not os.path.exists(target_file_path):\n","             shutil.copy2(source_file_path, target_file_path)\n","             copied_count += 1\n","    except FileNotFoundError:\n","        # CSV에 경로가 있지만 실제 Drive에 파일이 없는 경우 건너뜁니다.\n","        print(f\"[경고] 원본 파일이 Drive에 없습니다. 건너뜀: {source_file_path}\")\n","    except Exception as e:\n","        print(f\"[오류] 복사 중 예외 발생 ({relative_path}): {e}\")\n","\n","print(f\"✅ 로컬 복사 완료. 총 {copied_count}개의 파일 복사됨.\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1Zd89dDrACq","executionInfo":{"status":"ok","timestamp":1765098486857,"user_tz":-540,"elapsed":7,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}},"outputId":"03aef512-8e8e-4434-e255-50f3a16d177c"},"outputs":[{"output_type":"stream","name":"stdout","text":["새로운 이미지 루트 경로: /content/Images\n"]}],"source":["# =========================================================\n","# 3. IMG_ROOT_DIR 변수를 로컬 경로로 변경\n","# =========================================================\n","# 기존 경로 변수를 새로운 로컬 경로로 덮어씁니다.\n","IMG_ROOT_DIR = LOCAL_IMG_ROOT\n","\n","print(f\"새로운 이미지 루트 경로: {IMG_ROOT_DIR}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YvjqWS-dlG6","executionInfo":{"status":"ok","timestamp":1765098486877,"user_tz":-540,"elapsed":18,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}},"outputId":"71911423-2194-4f91-9868-a72f9e5b2f79"},"outputs":[{"output_type":"stream","name":"stdout","text":["총 unique item_id 개수: 1467\n"]}],"source":["# ============================================\n","# item_id 문자열 → 숫자 라벨 변환 매핑 생성\n","# ============================================\n","\n","df_full = pd.read_csv(CSV_PATH_LIGHT)\n","unique_ids = df_full[\"item_id\"].unique()\n","\n","id2label = {id_str: idx for idx, id_str in enumerate(unique_ids)}\n","print(\"총 unique item_id 개수:\", len(id2label))\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9AwRPQiJMPQr","executionInfo":{"status":"ok","timestamp":1765098486939,"user_tz":-540,"elapsed":54,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# CLAHE & Gray World\n","# -------------------\n","import numpy as np\n","import cv2\n","from PIL import Image\n","from torchvision import transforms\n","\n","def apply_clahe_pil(img: Image.Image) -> Image.Image:\n","    arr = np.array(img)\n","    yuv = cv2.cvtColor(arr, cv2.COLOR_RGB2YUV)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    yuv[:,:,0] = clahe.apply(yuv[:,:,0])\n","    out = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)\n","    return Image.fromarray(out)\n","\n","def apply_grayworld_pil(img: Image.Image) -> Image.Image:\n","    img_arr = np.array(img).astype(np.float32)\n","    mean_channels = img_arr.mean(axis=(0, 1), keepdims=True)\n","    gray_mean = mean_channels.mean()\n","    img_arr = img_arr * (gray_mean / (mean_channels + 1e-6))\n","    img_arr = np.clip(img_arr, 0, 255).astype(np.uint8)\n","    return Image.fromarray(img_arr)\n","\n","# -------------------\n","# Transform Builder\n","# -------------------\n","def build_transforms(domain_mode=\"baseline\", train=True, size=224):\n","    ops = []\n","\n","    # resizing + augmentation\n","    ops.append(transforms.Resize((size, size)))\n","    if train:\n","        ops.append(transforms.RandomHorizontalFlip(0.5))\n","        ops.append(transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2))\n","\n","    # domain normalization\n","    if domain_mode == \"grayworld\":\n","        ops.append(transforms.Lambda(apply_grayworld_pil))\n","    elif domain_mode == \"clahe\":\n","        ops.append(transforms.Lambda(apply_clahe_pil))\n","    elif domain_mode == \"both\":\n","        ops.append(transforms.Lambda(apply_clahe_pil))\n","        ops.append(transforms.Lambda(apply_grayworld_pil))\n","    # baseline: 아무것도 적용 안 함\n","\n","    # tensor + normalize\n","    ops.append(transforms.ToTensor())\n","    ops.append(transforms.Normalize(mean=[0.485,0.456,0.406],\n","                                    std=[0.229,0.224,0.225]))\n","\n","    return transforms.Compose(ops)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8x5cLRLUJmLE","executionInfo":{"status":"ok","timestamp":1765098486951,"user_tz":-540,"elapsed":7,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Dataset\n","# -------------------\n","class DeepFashionC2S(torch.utils.data.Dataset):\n","    def __init__(self, csv_path, img_root, transform=None, split='train'):\n","        self.df = pd.read_csv(csv_path)\n","        self.df = self.df[self.df['split']==split].reset_index(drop=True)\n","        self.img_root = img_root\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def load_crop(self, img_path, x1, y1, x2, y2):\n","        full_path = os.path.join(self.img_root, img_path)\n","        if not os.path.exists(full_path):\n","            raise FileNotFoundError(f\"File not found: {full_path}\")\n","        img = Image.open(full_path).convert(\"RGB\")\n","        return img.crop((x1, y1, x2, y2))\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        try:\n","            cons_img = self.load_crop(row['consumer_path'], row['cons_x1'], row['cons_y1'], row['cons_x2'], row['cons_y2'])\n","            shop_img = self.load_crop(row['shop_path'], row['shop_x1'], row['shop_y1'], row['shop_x2'], row['shop_y2'])\n","            if self.transform:\n","                cons_img = self.transform(cons_img)\n","                shop_img = self.transform(shop_img)\n","            return {\"consumer\": cons_img, \"shop\": shop_img, \"item_id\": row[\"item_id\"]}\n","        except FileNotFoundError:\n","            print(f\"[WARNING] File missing at idx {idx}. Skipping.\")\n","            return None"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TU0u3b3c1IV4","executionInfo":{"status":"ok","timestamp":1765098486955,"user_tz":-540,"elapsed":3,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Feature Embedding (EfficientNet-B3)\n","# -------------------\n","class FeatureEmbedding(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super().__init__()\n","        base_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n","        num_ftrs = base_model.classifier[1].in_features\n","        self.feature_extractor = base_model.features\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.final_fc = nn.Linear(num_ftrs, embedding_dim)\n","        self.bn = nn.BatchNorm1d(embedding_dim)\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.final_fc(x)\n","        x = self.bn(x)\n","        x = F.normalize(x, p=2, dim=1)\n","        return x.to(DEVICE)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"KagI8InF1Rfv","executionInfo":{"status":"ok","timestamp":1765098486967,"user_tz":-540,"elapsed":6,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Loss: Batch-Hard Triplet\n","# -------------------\n","def pairwise_distance_sq(embeddings):\n","    dot_product = torch.matmul(embeddings, embeddings.t())\n","    square_norm = torch.diag(dot_product)\n","    distances = square_norm.unsqueeze(0) - 2*dot_product + square_norm.unsqueeze(1)\n","    distances[distances<0] = 0\n","    return distances\n","\n","def batch_hard_triplet_loss(embeddings, labels, margin):\n","    pair_dist = pairwise_distance_sq(embeddings)\n","    labels_eq = labels.unsqueeze(0).eq(labels.unsqueeze(1))\n","    dist_ap = pair_dist.clone(); dist_ap[~labels_eq] = float('-inf')\n","    dist_an = pair_dist.clone(); dist_an[labels_eq] = float('inf')\n","    hardest_positive_dist, _ = dist_ap.max(dim=1)\n","    hardest_negative_dist, _ = dist_an.min(dim=1)\n","    triplet_loss = torch.clamp(hardest_positive_dist - hardest_negative_dist + margin, min=0.0)\n","    num_hard_triplets = triplet_loss.gt(1e-16).float().sum()\n","    return triplet_loss.sum()/num_hard_triplets if num_hard_triplets>0 else (embeddings*0).sum()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_xT7M_TDMk87","executionInfo":{"status":"ok","timestamp":1765098487097,"user_tz":-540,"elapsed":128,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Custom collate_fn\n","# -------------------\n","def custom_collate_fn(batch: List[Dict]):\n","    batch = [item for item in batch if item is not None]\n","    if not batch:\n","        return None\n","    return default_collate(batch)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"tFFI2p_-C3i3","executionInfo":{"status":"ok","timestamp":1765098487098,"user_tz":-540,"elapsed":3,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Recall@K\n","# -------------------\n","def calculate_recall_at_k(model, dataloader, device, ks=[1,5,10]):\n","    model.eval()\n","    all_query, all_gallery, all_labels = [], [], []\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            consumer_imgs = batch[\"consumer\"]\n","            shop_imgs = batch[\"shop\"]\n","            item_ids = batch[\"item_id\"]\n","            if isinstance(item_ids, (list, tuple)):\n","                labels_tensor = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                labels_tensor = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","            query_embs = model(consumer_imgs.to(device)).cpu().numpy()\n","            gallery_embs = model(shop_imgs.to(device)).cpu().numpy()\n","            all_query.append(query_embs)\n","            all_gallery.append(gallery_embs)\n","            all_labels.append(labels_tensor.cpu().numpy())\n","    query_embs = np.concatenate(all_query, axis=0)\n","    gallery_embs = np.concatenate(all_gallery, axis=0)\n","    gallery_labels = np.concatenate(all_labels, axis=0)\n","    sims = cosine_similarity(query_embs, gallery_embs)\n","    recalls = {}\n","    for k in ks:\n","        topk_idx = np.argsort(-sims, axis=1)[:, :k]\n","        correct = sum([gallery_labels[i] in gallery_labels[topk_idx[i]] for i in range(len(gallery_labels))])\n","        recalls[f'R@{k}'] = correct / len(gallery_labels)\n","    return recalls"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jV_InoDJpujk","executionInfo":{"status":"ok","timestamp":1765098487099,"user_tz":-540,"elapsed":3,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# -------------------\n","# Train Loop\n","# -------------------\n","def get_checkpoint_paths():\n","    return os.path.join(CHECKPOINT_DIR, \"checkpoint.pth\"), os.path.join(CHECKPOINT_DIR, \"best.pth\")\n","\n","def save_checkpoint(model, optimizer, epoch, best_val_metric, patience_count, filename):\n","    os.makedirs(os.path.dirname(filename), exist_ok=True)\n","    torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),\n","                'optimizer_state_dict':optimizer.state_dict(),\n","                'best_val_metric':best_val_metric,\n","                'patience_count':patience_count}, filename)\n","\n","def load_checkpoint(model, optimizer, filename):\n","    if not os.path.exists(filename):\n","        return 0, 0.0, 0\n","    checkpoint = torch.load(filename,map_location=DEVICE)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    return checkpoint['epoch']+1, checkpoint['best_val_metric'], checkpoint['patience_count']\n","\n","def train_model(train_dl, val_dl, experiment_name):\n","\n","    # -------------------\n","    # Prepare checkpoint path\n","    # -------------------\n","    ckpt_dir = os.path.join(CHECKPOINT_DIR, experiment_name)\n","    os.makedirs(ckpt_dir, exist_ok=True)\n","\n","    checkpoint_file = os.path.join(ckpt_dir, \"checkpoint.pth\")\n","    best_file = os.path.join(ckpt_dir, \"best.pth\")\n","    history_file = os.path.join(ckpt_dir, \"history.csv\")\n","\n","\n","    # -------------------\n","    # Load previous history (APPEND mode)\n","    # -------------------\n","    if os.path.exists(history_file):\n","        old_history = pd.read_csv(history_file)\n","        history = old_history.to_dict(\"records\")\n","        print(f\"Loaded previous history: {len(history)} records\")\n","    else:\n","        history = []\n","        print(\"Starting new history file\")\n","\n","    # -------------------\n","    # Model & Optimizer\n","    # -------------------\n","    model = FeatureEmbedding(EMBEDDING_DIM).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    start_epoch, best_val_metric, patience_counter = load_checkpoint(model, optimizer, checkpoint_file)\n","\n","    for epoch in range(start_epoch, MAX_EPOCHS):\n","\n","        model.train()\n","        total_loss = 0\n","\n","        for batch in tqdm(train_dl, leave=False):\n","            consumer_imgs = batch[\"consumer\"].to(DEVICE)\n","            shop_imgs = batch[\"shop\"].to(DEVICE)\n","            item_ids = batch[\"item_id\"]\n","\n","            if isinstance(item_ids, (list, tuple)):\n","                labels = torch.tensor([id2label[i] for i in item_ids], dtype=torch.long)\n","            else:\n","                labels = torch.tensor([id2label[item_ids]], dtype=torch.long)\n","\n","            labels = labels.to(DEVICE)\n","\n","            all_imgs = torch.cat([consumer_imgs, shop_imgs], dim=0)\n","            all_labels = torch.cat([labels, labels], dim=0)\n","\n","            optimizer.zero_grad()\n","            embeddings = model(all_imgs)\n","            loss = batch_hard_triplet_loss(embeddings, all_labels, TRIPLET_MARGIN)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_dl)\n","\n","        # -------------------\n","        # Validation\n","        # -------------------\n","        val_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1,5,10])\n","        val_metric = val_recalls[\"R@5\"]\n","\n","        history.append({\n","            \"epoch\": epoch + 1,\n","            \"train_loss\": avg_loss,\n","            \"val_R@1\": val_recalls[\"R@1\"],\n","            \"val_R@5\": val_recalls[\"R@5\"],\n","            \"val_R@10\": val_recalls[\"R@10\"],\n","        })\n","\n","        # Save full history (overwrite but includes appended records)\n","        pd.DataFrame(history).to_csv(history_file, index=False)\n","\n","        # -------------------\n","        # Check improvement\n","        # -------------------\n","        if val_metric > best_val_metric:\n","            best_val_metric = val_metric\n","            patience_counter = 0\n","            save_checkpoint(model, optimizer, epoch, best_val_metric, patience_counter, best_file)\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","                break\n","\n","        # Save intermediate checkpoint\n","        save_checkpoint(model, optimizer, epoch, best_val_metric, patience_counter, checkpoint_file)\n","\n","\n","    # -------------------\n","    # Load Best Model\n","    # -------------------\n","    load_checkpoint(model, optimizer, best_file)\n","\n","    final_recalls = calculate_recall_at_k(model, val_dl, DEVICE, ks=[1,5,10])\n","    print(f\"Final Best R@1: {final_recalls['R@1']:.4f}, \"\n","          f\"R@5: {final_recalls['R@5']:.4f}, \"\n","          f\"R@10: {final_recalls['R@10']:.4f}\")\n","\n","    # 이걸 실험 루프에서 받아서 저장함\n","    return final_recalls[\"R@1\"], final_recalls[\"R@5\"], final_recalls[\"R@10\"]"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"z_xd208wa2DJ","executionInfo":{"status":"ok","timestamp":1765098487102,"user_tz":-540,"elapsed":1,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["NUM_WORKERS = 2"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"-adzR9y6mVUw","executionInfo":{"status":"ok","timestamp":1765098487104,"user_tz":-540,"elapsed":1,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[],"source":["# DOMAIN_MODES = [\"none\", \"grayworld\", \"clahe\", \"both\"]\n","DOMAIN_MODES = [\"both\"]"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"2_qKOH13sRX1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0459473b-dac8-48f9-d9cd-9d3210320431","executionInfo":{"status":"ok","timestamp":1765100655700,"user_tz":-540,"elapsed":2168597,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Running: both =====\n","Loaded previous history: 16 records\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 47.2M/47.2M [00:00<00:00, 131MB/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n","100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n","100%|██████████| 35/35 [00:11<00:00,  2.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 24\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Final Best R@1: 0.6117, R@5: 0.6782, R@10: 0.7284\n"]}],"source":["results = {}\n","\n","for mode in DOMAIN_MODES:\n","    print(f\"\\n===== Running: {mode} =====\")\n","\n","    train_transform = build_transforms(domain_mode=mode, train=True)\n","    val_transform   = build_transforms(domain_mode=mode, train=False)\n","\n","    train_ds = DeepFashionC2S(csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR, transform=train_transform, split=\"train\")\n","    val_ds   = DeepFashionC2S(csv_path=CSV_PATH_LIGHT, img_root=IMG_ROOT_DIR, transform=val_transform, split=\"val\")\n","\n","    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n","                          num_workers=NUM_WORKERS, pin_memory=True,\n","                          collate_fn=custom_collate_fn)\n","    val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n","                          num_workers=NUM_WORKERS, pin_memory=True,\n","                          collate_fn=custom_collate_fn)\n","\n","    best_r1, best_r5, best_r10 = train_model(\n","        train_dl, val_dl,\n","        experiment_name=f\"domain_{mode}\"\n","    )\n","\n","    results[mode] = {\n","        \"R1\": best_r1,\n","        \"R5\": best_r5,\n","        \"R10\": best_r10\n","    }"]},{"cell_type":"code","source":["base_dir = \"/content/drive/MyDrive/2025CV/checkpoints_C\"\n","modes = [\"domain_none\", \"domain_grayworld\", \"domain_clahe\", \"domain_both\"]\n","\n","best_results = []\n","\n","for mode in modes:\n","  csv_path = os.path.join(base_dir, mode, \"history.csv\")\n","  df = pd.read_csv(csv_path)\n","\n","  best_r1_epoch = df[\"val_R@1\"].idxmax()\n","  best_r5_epoch = df[\"val_R@10\"].idxmax()\n","  best_r10_epoch = df[\"val_R@10\"].idxmax()\n","\n","  best_results.append({\n","      \"mode\": mode,\n","      \"best_R1_epoch\": int(df.loc[best_r1_epoch, \"epoch\"]),\n","      \"best_R1_value\": float(df.loc[best_r1_epoch, \"val_R@1\"]),\n","      \"best_R5_epoch\": int(df.loc[best_r5_epoch, \"epoch\"]),\n","      \"best_R5_value\": float(df.loc[best_r5_epoch, \"val_R@5\"]),\n","      \"best_R10_epoch\": int(df.loc[best_r10_epoch, \"epoch\"]),\n","      \"best_R10_value\": float(df.loc[best_r10_epoch, \"val_R@10\"]),\n","  })\n","\n","pd.DataFrame(best_results)"],"metadata":{"id":"TZcunJ2jXx0g","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1765102324706,"user_tz":-540,"elapsed":51,"user":{"displayName":"YOUNGCHAE SONG","userId":"13289283386529544862"}},"outputId":"b94d3532-1a13-4995-dfd6-84e17e7f1d3d"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["               mode  best_R1_epoch  best_R1_value  best_R5_epoch  \\\n","0       domain_none              7       0.596171              9   \n","1  domain_grayworld             11       0.610757             12   \n","2      domain_clahe             20       0.602552             18   \n","3       domain_both             19       0.611668             15   \n","\n","   best_R5_value  best_R10_epoch  best_R10_value  \n","0       0.644485               9        0.696445  \n","1       0.682771              12        0.736554  \n","2       0.672744              18        0.731085  \n","3       0.669098              15        0.729262  "],"text/html":["\n","  <div id=\"df-acd9b9eb-3a52-4dfa-8ed7-a09fd095e8c7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mode</th>\n","      <th>best_R1_epoch</th>\n","      <th>best_R1_value</th>\n","      <th>best_R5_epoch</th>\n","      <th>best_R5_value</th>\n","      <th>best_R10_epoch</th>\n","      <th>best_R10_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>domain_none</td>\n","      <td>7</td>\n","      <td>0.596171</td>\n","      <td>9</td>\n","      <td>0.644485</td>\n","      <td>9</td>\n","      <td>0.696445</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>domain_grayworld</td>\n","      <td>11</td>\n","      <td>0.610757</td>\n","      <td>12</td>\n","      <td>0.682771</td>\n","      <td>12</td>\n","      <td>0.736554</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>domain_clahe</td>\n","      <td>20</td>\n","      <td>0.602552</td>\n","      <td>18</td>\n","      <td>0.672744</td>\n","      <td>18</td>\n","      <td>0.731085</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>domain_both</td>\n","      <td>19</td>\n","      <td>0.611668</td>\n","      <td>15</td>\n","      <td>0.669098</td>\n","      <td>15</td>\n","      <td>0.729262</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acd9b9eb-3a52-4dfa-8ed7-a09fd095e8c7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-acd9b9eb-3a52-4dfa-8ed7-a09fd095e8c7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-acd9b9eb-3a52-4dfa-8ed7-a09fd095e8c7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-057cb669-155b-4e3b-a4fa-35dedda4c603\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-057cb669-155b-4e3b-a4fa-35dedda4c603')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-057cb669-155b-4e3b-a4fa-35dedda4c603 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"domain_grayworld\",\n          \"domain_both\",\n          \"domain_none\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R1_epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 7,\n        \"max\": 20,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          11,\n          19,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R1_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00733050020164984,\n        \"min\": 0.5961713764813127,\n        \"max\": 0.6116681859617138,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6107566089334548,\n          0.6116681859617138,\n          0.5961713764813127\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R5_epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 9,\n        \"max\": 18,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          12,\n          15,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R5_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016255747037490467,\n        \"min\": 0.6444849589790337,\n        \"max\": 0.682771194165907,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.682771194165907,\n          0.6690975387420237,\n          0.6444849589790337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R10_epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 9,\n        \"max\": 18,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          12,\n          15,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_R10_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01819351854138278,\n        \"min\": 0.6964448495897904,\n        \"max\": 0.7365542388331814,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7365542388331814,\n          0.7292616226071102,\n          0.6964448495897904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"tZ75ZzaoSlxd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
